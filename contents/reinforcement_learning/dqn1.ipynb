{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Deep Reinforced Learning*\n",
    "\n",
    "Hoy en día el estado del arte en el reconocimiento de patrones está dominado por las **redes neuronales profundas**\n",
    "\n",
    "- Visión Computacional: Redes Neuronales Convolucionales, Adversarios generativos\n",
    "- Reconocimiento de habla: Redes Recurrentes, WaveNet\n",
    "- Procesamiento de lenguaje Natural: Transformers\n",
    "\n",
    "Las redes neuronales son excelentes para representar el mundo: modelos\n",
    "\n",
    "> Podemos aprovechar esta capacidad para diseñar mejores algoritmos de aprendizaje reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value Learning:** En este tipo de algoritmos usamos un política de máxima utilidad para escoger acciones\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\text{arg} \\max_{a\\in \\mathcal{A}} Q(s, a)\n",
    "$$\n",
    "\n",
    "Y el problema entonces se reduce a aprender **Q**, *e.g* Q-Learning\n",
    "\n",
    "Sin embargo Q-learning tiene limitaciones: \n",
    "\n",
    "- requiere de heurísticas para explorar \n",
    "- espacio de acciones debe ser discreto\n",
    "- espacio de estados debe ser discreto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instalación recomendada**\n",
    "\n",
    "Utilice conda y pip\n",
    "\n",
    "    conda create -n RL python=3.9 pip matplotlib scipy scikit-learn cython jupyter ipykernel tornado==6.1 -c conda-forge\n",
    "    conda activate RL\n",
    "    pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "    pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente ambiente utiliza un ROM del clásico juego [Pong](https://en.wikipedia.org/wiki/Pong). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phuijse/.conda/envs/RL/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\") \n",
    "env.reset()\n",
    "\n",
    "a = env.action_space.sample()\n",
    "s, r, terminated, truncated, info = env.step(a)\n",
    "image = env.render() \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son las dimensiones del estado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir una imagen a color de 210x160 con píxeles entre 0 y 255. Significaría una cantidad de filas en la tabla Q de:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "255^{210\\cdot160\\cdot3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproximación de funciones\n",
    "\n",
    "El espacio de estados del ejemplo anterior es imposible de mantener en una tabla Q. Cabe destacar que este espacio de estados está aun lejos de un problema del mundo real.\n",
    "\n",
    "> Nos va a faltar memoría para guardar la tabla y datos para poder entrenar nuestro agente\n",
    "\n",
    "¿Qué podemos hacer?\n",
    "\n",
    "> Usar una representación más compacta para Q\n",
    "\n",
    "En lugar de tener una tabla con todos las combinaciones estado/acción podemos aproximar **Q** usando un **modelo paramétrico**. Esta es la idea principal tras *Value function approximation* (VFA) y *Q function approximation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El caso más sencillo es usar un **modelo lineal en sus parámetros**\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat Q_\\theta(s,a) &= \\theta_0 \\phi_0(s, a) + \\theta_1 \\phi_1(s, a) + \\theta_2 \\phi_2(s, a) + \\ldots + \\theta_M \\phi_M (s,a) \\\\\n",
    "&= \\sum_{j=0}^M \\theta_j \\phi_j (s,a) \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $\\{ \\theta\\}$ es un vector de parámetros con $M+1$ componentes\n",
    "- $\\{\\phi\\}$ es un conjunto de funciones base, *e.g.* polinomios, Fourier, árbol de decisión, kernels\n",
    "\n",
    "En lugar de aprender $Q$ explicitamente el objetivo es aprender $\\theta$.\n",
    "\n",
    ":::{important}\n",
    "\n",
    "La cantidad de parámetros es ahora independiente de la dimensionalidad del estado\n",
    "\n",
    ":::\n",
    "\n",
    "Al igual que antes nuestro objetivo es acercanos a la solución de la Ecuación de Bellman. Podemos escribir esto como el siguiente problema de optimización\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\| R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} \\hat Q_\\theta( s',a') - \\hat Q_\\theta(s,a)\\|^2\n",
    "$$\n",
    "\n",
    "de donde podemos aprender $\\theta$ iterativamente usando usando gradiente descendente \n",
    "\n",
    "$$\n",
    "\\theta_j \\leftarrow \\theta_j + 2 \\alpha \\left(R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} \\hat Q_\\theta (s',a') - \\hat Q_\\theta(s,a) \\right) \\phi_j(s,a)\n",
    "$$\n",
    "\n",
    "Sin embargo, un modelo lineal podría ser muy limitado. A continuación veremos como usar redes profundas para aproximar la función Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network (DQN) \n",
    "\n",
    "En [(Minh et al. 2013)](https://arxiv.org/abs/1312.5602) se usaron redes neuronales profundas de tipo convolucional para resolver una serie de juegos de ATARI con Aprendizaje Reforzardo obteniendo [desempeño sobre-humano en muchos de las pruebas](https://deepmind.com/blog/article/deep-reinforcement-learning). El modelo, llamado *Deep Q-network*, utiliza como estado el valor de todos los píxeles de cuatro cuadros consecutivos.\n",
    "\n",
    "La idea clave es\n",
    "\n",
    "> Aprovechar la capacidad de las redes neuronales profundas para representar datos complejos, e.g. imágenes \n",
    "\n",
    "o más en concreto\n",
    "\n",
    "> Aproximar la función Q usando una red convolucional entrenada directamente sobre los píxeles\n",
    "\n",
    "Veremos primero una formulación general y luego su aplicación al caso de imágenes.\n",
    "\n",
    "**Modelo**\n",
    "\n",
    "- La entrada de la red es el estado $s$. El vector de estado puede tener valores continuos o discretos\n",
    "- La salida de la red neuronal son los valores $Q(s, a_1), Q(s, a_2), \\ldots, Q(s, a_N)$\n",
    "    - Se considera un espacio de acciones discreto\n",
    "    - Esto es más eficiente que considerar $a'$ como una entrada y retornar $Q(s, a')$\n",
    "- La cantidad y tipo de las capas intermedias es decisión del usuario\n",
    "    - Si tenemos datos continuos (atributos) usamos capas completamente conectadas\n",
    "    - Si usamos píxeles es natural usar capas convolucionales\n",
    "- La función de perdida que se ocupa en DQN es el error cuadrático medio entre la ecuación de Bellman y la predicción de la red\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}\\left[\\left \\| R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} Q_\\theta(s', a') - Q_\\theta(s, a)\\right \\|^2\\right]\n",
    "$$\n",
    "\n",
    "Recordemos: $s'$ es el estado al que llegamos luego de ejecutar $a$ sobre $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay y Memory replay\n",
    "\n",
    "El entrenamiento usando sólo una instancia es bastante ruidoso (y un poco lento). Se debe considerar también que estamos entrenando con muestras muy correlacionadas (no iid) y esto puede introducir sesgos el entrenamiento de una red neuronal.\n",
    "\n",
    "> Para entrenar la DQN adecuadamente  (Minh et al 2013) propone un astuto \"truco\" llamado *Experience replay*  \n",
    "\n",
    "Esto consiste en almacenar la historia del agente en una memoria llamada *replay memory*\n",
    "\n",
    "> Cada elemento en la memoria es una tupla $(s_t, a_t, r_{t+1}, s_{t+1})$\n",
    "\n",
    "Con esto se crean mini-batches en orden aleatorio para entrenar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Double DQN*\n",
    "\n",
    "En [(van Hasselt, Guer y Silver, 2015)](https://arxiv.org/pdf/1509.06461.pdf) los autores notaron un problema importante en DQN. \n",
    "\n",
    "> Cuando calculamos $Q(s, a)$ y $\\max Q(s', a')$ usando la misma red es muy posible que sobre-estimemos la calidad de nuestro objetivo. Adicionalmente si el objetivo cambia constantemente el entrenamiento será inestable\n",
    "\n",
    "La solución propuesta consiste en usar redes neuronales distintas para la escoger la acción y para calcular el objetivo (ecuación de Bellman)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi) = \\mathbb{E}\\left[\\left \\| R(s,a) + \\gamma Q_\\phi\\left(s', \\text{arg}\\max_{a' \\in \\mathcal{A}} Q_\\theta(s', a')\\right) - Q_\\theta(s, a)\\right\\|^2\\right]\n",
    "$$\n",
    "\n",
    "Se utiliza\n",
    "\n",
    "- $Q_\\theta$ con parámetros $\\theta$ para escoger la acción: *policy network*\n",
    "- $Q_\\phi$ con parámetros $\\phi$ para construir el objetivo: *target network*\n",
    "\n",
    "Ambas redes comparten arquitectura y cantidad de neuronas, sin embargo sólo se optimiza la *policy network*\n",
    "\n",
    "Después de un \"cierto número de épocas\" los parametros de la policy network \"se copian\" en la *target network*. ¿Cada cuantas épocas actualizo la *target network*? Otro hiper-parámetro para el algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double-DQN en ambiente con estado continuo\n",
    "\n",
    "Utilizaremos al ambiente [*LunarLander*](https://gymnasium.farama.org/environments/box2d/lunar_lander/), donde el objetivo es aterrizar el módulo lunar entre las banderas amarillas. El espacio de acciones es discreto: \n",
    "\n",
    "- 0: No actuar\n",
    "- 1: Activar cohete izquierdo\n",
    "- 2: Activar cohete principal\n",
    "- 3: Activar cohete derecho\n",
    "\n",
    "El espacio de estados es un vector de 8 dimensiones:\n",
    "\n",
    "- Posición del módulo \n",
    "- Velocidad del módulo\n",
    "- Ángulo y velocidad angular del módulo\n",
    "- Booleanos que indican si las patas izquierda y derecha tocaron el suelo, respectivamente\n",
    "\n",
    "El episodio termina si el módulo se estrella o aluniza fuera de las banderas. La recompensa aumenta a medida que el módulo se acerca a la plataforma. La recompensa disminuye con el ángulo y con cada vez que dispara los cohetes.\n",
    "\n",
    "A continuación se muestre un agente con política aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "terminated, truncated = False, False\n",
    "while True:\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    obs, rewards, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrenar una Double Deep Q Network para resolver este problema utilizando la implementación de la librería `stable_baselines3`\n",
    "\n",
    "    pip install stable-baselines3[extra]\n",
    "\n",
    "La clase [DQN](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html), recibe:\n",
    "\n",
    "- Un string indicando el tipo de modelo que implementará la policy, por ejemplo \"MlpPolicy\" para un modelo de dos capas completamente conectadas o \"ConvPolicy\" para un modelo convolucional\n",
    "- El ambiente\n",
    "- Los hiperparámetros del entrenamiento, entre ellos el factor de descuento (`gamma`), la tasa de aprendizaje, el tamaño del replay memory (`buffer_size`), el tamaño de batch, el trade-off entre explotación y exploración (epsilon-greedy) y la frecuencia de actualización del modelo *target* (`target_update_interval`).\n",
    "\n",
    "El método tiene una función `learn` que realiza el entrenamiento. Podemos visualizar las curvas la *loss*, recompensa y largo de episodio utilizando tensorboard:\n",
    "\n",
    "    tensorboard --logdir=/tmp/tensorboard/\n",
    "    \n",
    "Y luego abriendo: http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to /tmp/tensorboard/dqn_lunar_lander/DQN_3\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | -203     |\n",
      "|    exploration_rate | 0.664    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 117      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 448      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.616    |\n",
      "|    n_updates        | 444      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 252      |\n",
      "|    ep_rew_mean      | -148     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 108      |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 2014     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 2012     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 407      |\n",
      "|    ep_rew_mean      | -156     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 106      |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 4884     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 1.23     |\n",
      "|    n_updates        | 4880     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 555      |\n",
      "|    ep_rew_mean      | -142     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 106      |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 8884     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.549    |\n",
      "|    n_updates        | 8880     |\n",
      "----------------------------------\n",
      "CPU times: user 6min 13s, sys: 478 ms, total: 6min 14s\n",
      "Wall time: 1min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fe83367ec10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"/tmp/tensorboard/dqn_lunar_lander/\",\n",
    "            gamma=0.99, learning_rate=5e-4, batch_size=128, buffer_size=50_000, \n",
    "            target_update_interval=250, train_freq=4, gradient_steps=-1,\n",
    "            learning_starts=0, exploration_fraction=0.12, exploration_final_eps=0.1, \n",
    "            policy_kwargs={'net_arch': [256, 256]})\n",
    "\n",
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agente entrenado en acción (se abrirá en una ventana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "terminated, truncated = False, False\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios**\n",
    "\n",
    "\n",
    "¿Por qué a veces la *loss* sube? \n",
    "\n",
    "Recordemos que el \"training set\" está cambiando con cada paso. Es normal ver que los *loss* tenga subidas abruptas. En cierto modo estas subidas son deseables pues significa que nuestro algoritmo está \"sorprendiendose\" con nuevos estados\n",
    "\n",
    "¿Qué significa si la recompensa promedio se fue en picada?\n",
    "\n",
    "Lo que está ocurriendo es un problema común en deep RL que se llama: **\"olvido catastrófico\"**. El modelo es cada vez más bueno resolviendo el problema y la memoria se llena de \"buenas transiciones\". Por lo tanto los *minibatches* ya no tienen ejemplos erroneos y el modelo ya no sabe lo que está mal.\n",
    "\n",
    "Se puede combatir con *prioritized replay memory* o con otros heurísticas:\n",
    "\n",
    "- Reservar un espacio en la memoria para las transiciones (de exploración) iniciales \n",
    "- **Early stopping:** Guardar el modelo de mayor recompensa y detener el aprendizaje si la recompensa no sube en un cierto número de episodios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Rainbow DQN](https://arxiv.org/pdf/1710.02298.pdf)\n",
    "\n",
    "Rainbow DQN es un trabajo que reune todos los avances relacionados a DQN\n",
    "\n",
    "1. *Double DQN*: Usar un modelo separado para calcular los targets\n",
    "1. *Replay memory*: Almacenar experiencias pasadas y muestrear aleatoriamente de la memoria para disminuir la correlación \n",
    "1. [*Prioritized experience replay*](https://arxiv.org/abs/1511.05952): El minibatch se construye ponderando las experiencias anteriores en función del error. Experiencias con más error tienen más probabilidad de ser escogidas. \"Se puede aprender más de los ejemplos más difíciles\"\n",
    "1. [*Dueling networks*](https://arxiv.org/abs/1511.06581): Se separa la función $Q(s,a) = V(s) + A(s,a)$, donde cada función es una red neuronal que comparten capas. Con esto se puede aprender el valor de cada estado de forma desacoplada a la importancia relativa de las acciones\n",
    "1. [*Noisy nets*](https://arxiv.org/abs/1706.10295): Capas de redes neuronales que incluyen un ruido paramétrico en sus parámetros lo que permite explorar sin necesidad de la heurística $\\epsilon$-greddy\n",
    "1. entre otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
