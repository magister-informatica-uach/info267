{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import animation\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de decisión y Métodos de ensamble\n",
    "\n",
    "Slides [aquí](https://docs.google.com/presentation/d/1pxJk4cpI_gpvLhDi86EISHjggdyD95K6PgwKlJplkTg/edit?usp=sharing)\n",
    "\n",
    "Material adicional: Capítulos 10 (boosting) y 15 (bagging, random forest) de \"Elements of Statistical Learning\" (ver README del repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "N = 1000  \n",
    "X, Y = make_moons(n_samples=N, noise=0.35)\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c='b', marker='o', \n",
    "           s=10, alpha=0.5, label='class 1')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c='r', marker='x', \n",
    "           s=10, alpha=0.5, label='class 2')\n",
    "plt.legend()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.75, test_size=0.25)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Árbol de decisión\n",
    "\n",
    "- Secuencia de operadores relacionales sobre los atributos en forma de árbol\n",
    "- Los nodos \"hoja\" están asociados a una etiqueta (clasificación)\n",
    "- Los nodos intermedios separan los datos (*splits*)\n",
    "- Las separaciones se seleccionan usando la ganancia de información (*entropy*) o el índice de gini \n",
    "\n",
    "En *scikit learn* usamos el módulo [`tree`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) que tiene árboles para clasificación y regresión\n",
    "\n",
    "    sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None,\n",
    "                                        min_samples_split=2, min_samples_leaf=1, \n",
    "                                        in_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                        random_state=None, max_leaf_nodes=None, \n",
    "                                        min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                        class_weight=None, presort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudie como cambia el clasificador usando distintas profundidades máximas: parámetro `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), tight_layout=True)\n",
    "\n",
    "def update_plot(max_depth):\n",
    "    model = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=max_depth)\n",
    "    model.fit(X_train, Y_train)\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "        ax_.contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "    ax[0].set_title('Entrenamiento'); ax[1].set_title('Prueba')\n",
    "    ax[0].scatter(X_train[Y_train==0, 0], X_train[Y_train==0, 1], color='k', s=10, marker='o', alpha=0.5)\n",
    "    ax[0].scatter(X_train[Y_train==1, 0], X_train[Y_train==1, 1], color='k', s=10, marker='x', alpha=0.5)\n",
    "    ax[1].scatter(X_test[Y_test==0, 0], X_test[Y_test==0, 1], color='k', s=10, marker='o', alpha=0.5)\n",
    "    ax[1].scatter(X_test[Y_test==1, 0], X_test[Y_test==1, 1], color='k', s=10, marker='x', alpha=0.5)\n",
    "\n",
    "\n",
    "widgets.interact(update_plot, max_depth=IntSlider_nice(min=1, max=51));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La profundidad nos da complejidad\n",
    "- Mucha profundidad provoca sobreajuste\n",
    "\n",
    "\n",
    "> Usamos estrategias de validación para seleccionar la \"mejor\" profundidad\n",
    "\n",
    "Por ejemplo 5-fold cross-validation\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\"><img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"600\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Crear kfold generator\n",
    "fold_generator = KFold(n_splits=5)\n",
    "# para cada profundidad\n",
    "for max_depth in [1, 2, 3, 4, 5, 6, 8, 10, 15, 20]: \n",
    "    # crear un modelo\n",
    "    model = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', \n",
    "                                        max_depth=max_depth)\n",
    "    # crear 5 splits\n",
    "    score = np.zeros(shape=(5, ))\n",
    "    for fold_index, (train_index, valid_index) in enumerate(fold_generator.split(X_train)):\n",
    "        # entrenar en 4 folds\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        # validar en 1 fold\n",
    "        score[fold_index] = model.score(X_train[valid_index], Y_train[valid_index])\n",
    "    # mostrar promedio y desviación estándar de la loss\n",
    "    print(\"profundidad %d:\\tloss validación: %f +/- %f\" % (max_depth, np.mean(score), np.std(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos automatizar este procedimiento a más parámetros usando [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "Digamos que queremos encontrar el mejor criterio y la mejor profundidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'criterion':('entropy', 'gini'), 'max_depth':[1, 2, 3, 4, 5, 6, 8, 10, 15, 20]}\n",
    "model = tree.DecisionTreeClassifier(splitter='best')\n",
    "dts = GridSearchCV(model, params, cv=5)\n",
    "dts.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor árbol es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dts.best_estimator_\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3.5), tight_layout=True)\n",
    "\n",
    "print(\"Entrenamiento: %f\" % (model.score(X_train, Y_train)))\n",
    "print(\"Validación: %f\" % (model.score(X_test, Y_test)))\n",
    "\n",
    "ax[0].contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "ax[0].scatter(X[Y==0, 0], X[Y==0, 1], color='k', s=10, marker='o', alpha=0.5)\n",
    "ax[0].scatter(X[Y==1, 0], X[Y==1, 1], color='k', s=10, marker='x', alpha=0.5)\n",
    "fpr, tpr, th = roc_curve(Y_train, model.predict_proba(X_train)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Entrenamiento', linewidth=2)\n",
    "fpr, tpr, th = roc_curve(Y_test, model.predict_proba(X_test)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Prueba', linewidth=2)\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "ax[1].set_ylim([0.0, 1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Gradient Boosting  \n",
    "\n",
    "- Generaliza el concepto de boosting a cualquier función de costo derivable\n",
    "- Cada clasificador en la cadena se entrena con los residuos del clasificador anterior\n",
    "\n",
    "En scikit-learn en el módulo [`ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) se encuentra Gradient Boosting para clasificar y hacer regresión\n",
    "\n",
    "    sklearn.ensemble.GradientBoostingClassifier(loss=’deviance’, learning_rate=0.1, \n",
    "                                                n_estimators=100, subsample=1.0, \n",
    "                                                max_depth=3, ...)\n",
    "                                                \n",
    "Esta implementación usa árboles como clasificador débil\n",
    "\n",
    "Explicación de los parámetros (algunos):\n",
    "- n_estimators: Número de árboles\n",
    "- max_depth: Profundidad de los árboles\n",
    "- subsample: Se usa para que cada árbol use una submuestra del dataset\n",
    "- learning_rate: Se usa para disminuir la contribución de cada árbol sucesivo\n",
    "- max_features: Número de atributos a considerar en cada split (reduce la varianza)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontremos el mejor ensamble usando 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "params = {'loss':('deviance', 'exponential'), \n",
    "          'max_depth':[1, 5, 10, 20],\n",
    "          'n_estimators': [1, 10, 20, 50, 100]}\n",
    "model = ensemble.GradientBoostingClassifier(subsample=0.5, learning_rate=0.1, max_features=None)\n",
    "gbs = GridSearchCV(model, params, cv=5)\n",
    "gbs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = gbs.cv_results_['mean_test_score'][gbs.cv_results_['param_loss']== 'deviance']\n",
    "stds = gbs.cv_results_['std_test_score'][gbs.cv_results_['param_loss']== 'deviance']\n",
    "for mean, std, params in zip(means, stds, gbs.cv_results_['params']):\n",
    "    print(\"score: %0.3f (+/-%0.03f) con %r\"\n",
    "          % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor modelo es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gbs.best_estimator_\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting funciona bien con árboles poco profundo\n",
    "\n",
    "Clasificador débil con alto sesgo y baja varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3), tight_layout=True)\n",
    "\n",
    "ax[0].contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "ax[0].scatter(X[Y==0, 0], X[Y==0, 1], color='k', s=10, marker='o', alpha=0.5)\n",
    "ax[0].scatter(X[Y==1, 0], X[Y==1, 1], color='k', s=10, marker='x', alpha=0.5)\n",
    "fpr, tpr, th = roc_curve(Y_train, model.predict_proba(X_train)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Entrenamiento', linewidth=2)\n",
    "fpr, tpr, th = roc_curve(Y_test, model.predict_proba(X_test)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Prueba', linewidth=2)\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "ax[1].set_ylim([0.0, 1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comparar el rendimiento del mejor árbol con el mejor ensamble en el conjunto de Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 4), tight_layout=True)\n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR/Recall')\n",
    "\n",
    "Y_pred = dts.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, th = roc_curve(Y_test, Y_pred)\n",
    "ax.plot(fpr, tpr, label=\"Decision Tree %0.4f\" %auc(fpr, tpr), linewidth=2)\n",
    "\n",
    "Y_pred = gbs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, th = roc_curve(Y_test, Y_pred)\n",
    "ax.plot(fpr, tpr, label=\"Gradient boosting %0.4f\" %auc(fpr, tpr), linewidth=2)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Random Forest\n",
    "\n",
    "- Conjunto de árboles de decisión entrenados en paralelo usando bootstrap \n",
    "- Cada árbol se entrena con un **subconjunto aleatorio** de los datos (bagging)\n",
    "- Cada árbol se entrena con un **subconjunto aleatorio** de los atributos (random forest)\n",
    "\n",
    "Nuevamente en el módulo [`ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) podemos encontrar random forest para clasificar y hacer regresión\n",
    "\n",
    "    sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, \n",
    "                                            max_depth=None,\n",
    "                                            max_features=’auto’, bootstrap=True, \n",
    "                                            oob_score=False,\n",
    "                                            n_jobs=None, class_weight=None, ...)\n",
    "                                                \n",
    "\n",
    "Explicación de los parámetros (algunos):\n",
    "- n_estimators: Número de árboles\n",
    "- max_depth: Profundidad de los árboles\n",
    "- max_features: Número de atributos a considerar en cada split\n",
    "- criterion: Para decidir como se escogen los splits\n",
    "- bootstrap: Muestreo con reemplazo (desactiva bagging)\n",
    "- class_weight: Usar o no una mayor ponderación para las clases menos representadas\n",
    "- n_jobs: número de cores para entrenar\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion':('entropy', 'gini'),\n",
    "          'max_depth':[1, 5, 10, 20],\n",
    "          'n_estimators': [1, 10, 20, 50, 100]}\n",
    "model = ensemble.RandomForestClassifier(max_features=None, n_jobs=-1)\n",
    "rfs = GridSearchCV(model, params, cv=5)\n",
    "rfs.fit(X_train, Y_train)\n",
    "display(rfs.best_estimator_)\n",
    "display(rfs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor Random Forest es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rfs.best_estimator_\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia de GB, Random Forest prefiere árboles más profundos\n",
    "\n",
    "Clasificadores débiles con bajo sesgo y alta varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3), tight_layout=True)\n",
    "\n",
    "ax[0].contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "ax[0].scatter(X[Y==0, 0], X[Y==0, 1], color='k', s=10, marker='o', alpha=0.5)\n",
    "ax[0].scatter(X[Y==1, 0], X[Y==1, 1], color='k', s=10, marker='x', alpha=0.5)\n",
    "fpr, tpr, th = roc_curve(Y_train, model.predict_proba(X_train)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Entrenamiento', linewidth=2)\n",
    "fpr, tpr, th = roc_curve(Y_test, model.predict_proba(X_test)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Prueba', linewidth=2)\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "ax[1].set_ylim([0.0, 1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comparar el rendimiento del mejor árbol con los mejores ensambles Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 4), tight_layout=True)\n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR/Recall')\n",
    "\n",
    "Y_pred = dts.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, th = roc_curve(Y_test, Y_pred)\n",
    "ax.plot(fpr, tpr, label=\"Decision Tree %0.4f\" %auc(fpr, tpr), linewidth=2)\n",
    "\n",
    "Y_pred = gbs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, th = roc_curve(Y_test, Y_pred)\n",
    "ax.plot(fpr, tpr, label=\"Gradient boosting %0.4f\" %auc(fpr, tpr), linewidth=2)\n",
    "\n",
    "Y_pred = rfs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, th = roc_curve(Y_test, Y_pred)\n",
    "ax.plot(fpr, tpr, label=\"Random Forest %0.4f\" %auc(fpr, tpr), linewidth=2)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Más sobre ensambles\n",
    "\n",
    "Dos algoritmos de Gradient Boosting para árboles de decisión (GBDT) extremadamente competitivos:\n",
    "- [XGBoost](http://dmlc.cs.washington.edu/xgboost.html)\n",
    "- [LightGBM](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree) e [implementación oficial](https://lightgbm.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "Ambos implementan estrategias para mejorar la eficiencia y realizar cálculos paralelos/distribuidos e incluso usando GPU\n",
    "\n",
    "Estado del arte en clasificación de datos estructurados (tablas)\n",
    "\n",
    "[Randomer Forest?](https://arxiv.org/abs/1506.03410v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
