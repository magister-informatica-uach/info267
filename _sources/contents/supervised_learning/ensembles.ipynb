{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensambles secuenciales: Boosting\n",
    "\n",
    "Boosting es una familia de algoritmos que buscan combinar estimadores (clasificadores o regresores) débiles de forma secuencial (cadena) para construir un estimador fuerte\n",
    "\n",
    "Estimador débil\n",
    ": Algoritmo que produce un resultado (al menos) levemente mejor que el azar \n",
    "\n",
    "Estimador fuerte\n",
    ": Algoritmo que produce un resultado correcto en la mayoría de los ejemplos\n",
    "\n",
    ":::{epigraph}\n",
    "\n",
    "Cualquier estimador débil puede ser mejorado (*boosted*) a un estimador fuerte\n",
    "\n",
    "-- [Robert Shapire](https://link.springer.com/article/10.1007/BF00116037)\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procedimiento general de un algoritmo de tipo boosting es:\n",
    "\n",
    "1. Entrenar un estimador débil con toda la distribución de datos\n",
    "1. Crear una nueva distribución que le da más peso a los errores del clasificador débil anterior\n",
    "1. Entrenar otro estimador débil en la nueva distribución\n",
    "1. Combinar los estimadores débiles y volver a 2\n",
    "\n",
    "Estos pasos se muestran esquemáticamente en la siguiente figura para el caso particular de clasificación:\n",
    "\n",
    "<img src=\"img/boosting.png\" width=\"700\">\n",
    "\n",
    ":::{important}\n",
    "\n",
    "El clasificador $H_2$ se encarga de corregir los errores de $H_1$. La combinación de $H_1$ y $H_2$ es el clasificador fuerte\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matemáticamente:\n",
    "\n",
    ":::{prf:algorithm} Algoritmo general de Boosting\n",
    "\n",
    "**Entradas** Un conjunto de datos $\\mathcal{D}$ y un número máximo de estimadores $T$\n",
    "\n",
    "1. Definir conjunto inicial $D_{1} = D$\n",
    "1. Para $t = 1, \\ldots, T$:\n",
    "    1. Entrenar un estimador débil sobre $D_t$\n",
    "    1. Evaluar el error del estimador débil\n",
    "    1. Ponderar los ejemplos en base al error para crear $D_{t+1}$\n",
    "1. Combinar las salidas de los estimadores débiles \n",
    "\n",
    ":::\n",
    "\n",
    "En esta lección utilizaremos árboles de decisión como estimador débil. Esto define los pasos de entrenamiento y evaluación del algoritmo anterior. Sólo resta definir como\n",
    "\n",
    "- Crear $D_{t+1}$\n",
    "- Combinar los estimadores débiles\n",
    "\n",
    "A continuación veremos como definen estos puntos dos algoritmos particulares de Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting (Adaboost)\n",
    "\n",
    "[Adaboost](https://www.sciencedirect.com/science/article/pii/S002200009791504X) es un algoritmo diseñado para clasificación binaria $\\{-1,1\\}$ donde los clasificadores débiles se combinan linealmente como\n",
    "\n",
    "$$\n",
    "H_T(x) = \\sum_{t=1}^T \\alpha_t h_t(x)\n",
    "$$\n",
    "\n",
    "donde la clase predicha se obtiene aplicando la función signo sobre $H_T(x)$\n",
    "\n",
    "El ensamble se entrena minimizando la función de pérdida exponencial\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{L}(H_T) &= \\sum_{i=1}^N \\exp \\left (-y_i H_T(x_i) \\right) \\\\\n",
    "&= \\sum_{i=1}^N \\exp \\left (-y_i H_{T-1}(x_i) -y_i \\alpha_T h_T(x_i)\\right) \\\\\n",
    "&= \\sum_{i=1}^N w_i^{(T)}\\exp \\left (-y_i \\alpha_T h_T(x_i)\\right) \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Para entrenar el último clasificador de la secuencia $h_T$ podemos asumir $w_i^{(T)}$ constante\n",
    "\n",
    ":::\n",
    "\n",
    "Dividiendo la función de costo en los casos bien y mal clasificados\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{L}(H_T) &= \\sum_{i=1}^N w_i^{(T)}\\exp \\left (-y_i \\alpha_T h_T(x_i)\\right) \\\\\n",
    "&= \\sum_{h(x_i)y_i = 1} e^{-\\alpha_T} w_i^{(T)} + \\sum_{h(x_i)y_i \\neq 1} e^{\\alpha_T} w_i^{(T)} \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Derivando en función de $\\alpha$ se tiene\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2} \\log \\left(\\frac{1-\\epsilon_t}{\\epsilon_t} \\right),\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\n",
    "\\epsilon_t = \\frac{\\sum_{i=1}^N w_i^{(T)} \\mathbb{1}(h_t(x_i)\\neq y_i)}{\\sum_{i=1}^N w_i^{(T)}}\n",
    "$$\n",
    "\n",
    "donde $\\mathbb{1}()$ es la función indicadora que es 1 si su argumento es cierto o 0 en caso contrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto tenemos\n",
    "\n",
    ":::{prf:algorithm} Algoritmo Adaptive Boosting\n",
    "\n",
    "**Entradas** Un conjunto de datos $\\mathcal{D}$ y un número máximo de estimadores $T$\n",
    "\n",
    "1. Inicializar los pesos $w_i^{(1)} = 1/N$\n",
    "1. Para $t = 1, \\ldots, T$:\n",
    "    1. Entrenar un estimador débil $h_t$ sobre los datos ponderados por $w_i^{(t)}$\n",
    "    1. Calcular $\\alpha_t$\n",
    "    1. Actualizar los pesos $w_i^{(t+1)} = w_i^{(t)} \\exp (2 \\alpha_t \\mathbb{1}(h_t(x_i) \\neq y_i))$\n",
    "    \n",
    ":::\n",
    "\n",
    "El clasificador fuerte está totalmente definido por los clasificadores débiles $h_t$ y los ponderadores $\\alpha_t$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Los pesos de los datos se actualizan con los errores del último clasificador\n",
    "\n",
    ":::\n",
    "\n",
    ":::{important}\n",
    "\n",
    "Los ensambles de tipo boosting reducen progresivamente el sesgo (error) agregando secuencialmente clasificadores débiles\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "El algoritmo de [gradient boosting](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full) combina el método de gradiente descendente con el algoritmo general de boosting. Es capaz de hacer tanto clasificación como regresión y puede usar cualquier función de pérdida que sea derivable\n",
    "\n",
    "Sea una función de costo sobre un dataset $(x_i, y_i)_{i=1,\\ldots,N}$\n",
    "\n",
    "$$\n",
    "\\min \\sum_{i=1}^N L(y_i, H_T(x_i)),\n",
    "$$\n",
    "\n",
    "donde $H_T$ es el estimador fuerte\n",
    "\n",
    "Por ejemplo, en un problema de regresión, este se define como\n",
    "\n",
    "$$\n",
    "H_T(x_i) = \\sum_{t=1}^T h_t(x_i),\n",
    "$$\n",
    "\n",
    "es decir una suma de \"regresores\" débiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un problema de regresión se utiliza típicamente el error cuadrático medio como función de costo\n",
    "\n",
    "$$\n",
    "L(H_T(x_i), y_i) = \\frac{1}{2} \\left(y_i - H_T(x_i) \\right)^2\n",
    "$$\n",
    "\n",
    "Supongamos que tenemos $H_3 = h_1 + h_2 + h_3$ y deseamos agregar un nuevo estimador tal que\n",
    "\n",
    "$$\n",
    "H_4 = H_3 + h_4\n",
    "$$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Los estimadores se agregan de uno por uno de forma *greedy* \n",
    "\n",
    ":::\n",
    "\n",
    "Agregar el nuevo estimador debería acercar al estimador fuerte a la etiqueta es decir \n",
    "\n",
    "$$\n",
    "H_3 + h_4 \\approx y\n",
    "$$\n",
    "\n",
    "Para lograr esto el nuevo estimador $h_4$ se entrena **minimizando el residuo** $y-H_3$\n",
    "\n",
    "$$\n",
    "h_4 = \\text{arg} \\min_{h} \\sum_{i=1}^N L(h(x_i), y_i - H_3(x_i))\n",
    "$$\n",
    "\n",
    "Donde el residuo está relacionado a \n",
    "\n",
    "$$\n",
    "\\frac{dL(H_3(x_i), y_i)}{dH_3(x_i)} = H_3(x_i) - y_i\n",
    "$$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Si utilizamos el error cuadrático, entonces los residuos son equivalentes al negativo del gradiente, esta es la razón del nombre del algoritmo\n",
    "\n",
    ":::\n",
    "\n",
    "Para reducir el sobreajuste del ensamble (regularización) se agrega tipicamente\n",
    "\n",
    "$$\n",
    "H_{t+1} = H_t + \\nu h_t\n",
    "$$\n",
    "\n",
    "una constate $\\nu$ denominada tasa de aprendizaje. Esto disminuye la contribución de cada estimador débil (enlentece el entrenamiento)\n",
    "\n",
    ":::{note}\n",
    "\n",
    "En general una tasa de aprendizaje pequeña requerirá una mayor cantidad de clasificadores débiles. La ventaja de una tasa pequeña es que está relacionada a menores errores en el conjunto de test \n",
    "\n",
    ":::\n",
    "\n",
    "Para problemas de clasificación con $K$ clases se suele utilizar la siguiente función de costo, llamada generalmente *logarithmic loss* o *log loss*\n",
    "\n",
    "$$\n",
    "L(y_i, x_i) = \\sum_{k=1}^K y_{ik} \\log p_k (x_i)\n",
    "$$\n",
    "\n",
    "donde $y_i$ es un vector de largo K de tipo *one-hot* y $H(x)$ retorna también un vector de largo $K$ que se normaliza como\n",
    "\n",
    "$$\n",
    "p_k(x) = \\frac{e^{H^{(k)}(x)}}{ \\sum_{k=1}^K e^{H^{(k)}(x)} }\n",
    "$$\n",
    "\n",
    "tal que cada componente este en el rango $[0,1]$ y que además los $K$ sumen uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación en `scikit-learn`\n",
    "\n",
    "El módulo [`ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) de scikit-learn tiene implementaciones de *Gradient Boosting* para problemas de clasificación y regresión. Nos enfocaremos en la primera\n",
    "\n",
    "Los principales argumentos de [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) son\n",
    "\n",
    "- `loss`: La función de costo. Las opciones son `'log_loss'/'deviance'` (dependiendo de su versión de scikit-learn) o `'exponential'`\n",
    "- `n_estimators:` Cantidad de clasificadores débiles\n",
    "- `learning_rate`: Tasa de aprendizaje (no-negativo)\n",
    "- `subsample`: Booleano que indica si cada clasificador débil utiliza el dataset completo o una submuestra\n",
    "\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Si se utiliza `loss='exponential'` el algoritmo se vuelve equivalent a *AdaBoost* (sólo para clasificación binaria)\n",
    "\n",
    ":::\n",
    "\n",
    "También recibe argumentos relacionados a los clasificadores débiles (árboles), entre ellos:\n",
    "\n",
    "- `max_depth`: Profundidad máxima de los árboles\n",
    "- `min_samples_split`: Número mínimo de muestras para permitir un `split`\n",
    "\n",
    "El objeto tiene implementados los métodos usuales `fit`, `predict`, `predict_proba` y `decision_function`\n",
    "\n",
    "Adicionalmente cuenta con `staged_predict`, `staged_predict_proba` y `staged_decision_function`, que retornan las predicciones de los clasificadores débiles individuales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo** Entrenamiento de ensamble gradient boosting para clasificación de vino\n",
    "\n",
    "A continuación utilizaremos [`wine`](https://archive.ics.uci.edu/ml/datasets/wine), una base de datos con 13 atributos numéricos y 3 clases asociadas a vinos de distinto origen. Los atributos representan propiedades químicas de los vinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_struct = load_wine()\n",
    "X = data_struct.data\n",
    "y = data_struct.target\n",
    "X_names = data_struct.feature_names\n",
    "\n",
    "np.random.seed(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza una validación cruzada buscando los mejores hiperparámetros del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': [0.1, 0.2, 0.5],\n",
       "                         'max_depth': [1, 5, 10, 20],\n",
       "                         'n_estimators': [1, 10, 20, 50, 100]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'learning_rate': [0.1, 0.2, 0.5], \n",
    "          'max_depth': [1, 5, 10, 20],\n",
    "          'n_estimators': [1, 10, 20, 50, 100]}\n",
    "\n",
    "model = GradientBoostingClassifier(loss='deviance')\n",
    "validator = GridSearchCV(model, params, cv=3, refit=True)\n",
    "validator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mejores modelos de acuerdo a la validación cruzada son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.959930</td>\n",
       "      <td>0.022174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.959930</td>\n",
       "      <td>0.022174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951994</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951994</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.944057</td>\n",
       "      <td>0.044622</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_learning_rate param_max_depth param_n_estimators  mean_test_score  \\\n",
       "24                 0.2               1                100         0.959930   \n",
       "41                 0.5               1                 10         0.959930   \n",
       "21                 0.2               1                 10         0.951994   \n",
       "42                 0.5               1                 20         0.951994   \n",
       "23                 0.2               1                 50         0.944057   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "24        0.022174                1  \n",
       "41        0.022174                1  \n",
       "21        0.033398                3  \n",
       "42        0.033398                3  \n",
       "23        0.044622                5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "columns = [\"param_learning_rate\", \"param_max_depth\", \"param_n_estimators\", \n",
    "           \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "pd.DataFrame(validator.cv_results_)[columns].sort_values(by=\"rank_test_score\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso hay varios modelos que obtuvieron el primer lugar en términos de *accuracy* promedio\n",
    "\n",
    "`GridSearchCV` retorna arbitrariamente el primero en orden de ejecución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 100}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validator.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "Boosting funciona bien con árboles poco profundos. Los árboles de poca profundidad suelen tener alto sesgo y baja varianza\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado de predicción en el conjunto de test es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       1.00      0.95      0.98        22\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_gb = validator.best_estimator_\n",
    "print(classification_report(y_test, best_gb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    "\n",
    "Otros algoritmos de Boosting con árboles de decisión extremadamente competitivos:\n",
    "\n",
    "- [HistGradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)\n",
    "- [XGBoost](http://dmlc.cs.washington.edu/xgboost.html)\n",
    "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "Estos implementan estrategias para mejorar la eficiencia y realizar cálculos paralelos/distribuidos \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensambles paralelos\n",
    "\n",
    "Los métodos de ensambles paralelos entrenan clasificadores débiles de forma independiente (no secuecial) y luego combinan sus resultados\n",
    "\n",
    "El supuesto principal de estos métodos es que los clasificadores débiles tienen error de generalización independiente (de lo contrario no habría ganancia al combinarlos)\n",
    "\n",
    "Si definimos el error de generalización como\n",
    "\n",
    "$$\n",
    "\\epsilon_t = P(h_t(x) \\neq y)\n",
    "$$\n",
    "\n",
    "y asumiendo un clasificador binario fuerte con\n",
    "\n",
    "$$\n",
    "H(x) = \\text{signo} \\left( \\sum_{t=1}^T h_t(x) \\right)\n",
    "$$\n",
    "\n",
    "(voto por mayoría), entonces se puede mostrar que\n",
    "\n",
    "$$\n",
    "P(H(x) \\neq y) = \\sum_{k=0}^{T/2} \\binom{T}{k} (1-\\epsilon)^k \\epsilon^{T-k} \\leq \\exp \\left( -\\frac{1}{2} T(2\\epsilon -1)^2 \\right)\n",
    "$$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Si los errores de generalización son independientes, el error del clasificador fuerte se reduce exponencialmente con la cantidad de clasificadores débiles\n",
    "\n",
    ":::\n",
    "\n",
    "Pero en la práctica no es posible tener clasificadores independientes si están entrenados con el mismo conjunto de datos ¿Qué podemos hacer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging: Bootstrap Aggregating\n",
    "\n",
    "*Bootstrap* es una técnica estadística para obtener intervalos de confianza empíricos que se basa en obtener muestras distintas pero representativas del conjunto de datos original\n",
    "\n",
    "La técnica más clásica y simple para lograr lo anterior es **muestreo con reemplazo** y se ilustra en la siguiente figura\n",
    "\n",
    "<img src=\"img/bootstrap.png\" width=\"700\">\n",
    "\n",
    "El conjunto de datos original (izquierda) se remuestrea $T$ veces. En cada muestra se escogen aleatoriamente tantos ejemplos como existían en el conjunto original. Sin embargo algunos ejemplos no se escogen, mientras que otros se escogen más de una vez (reemplazo)\n",
    "\n",
    "\n",
    "El algoritmo de [Bagging](https://link.springer.com/article/10.1007/BF00058655) consiste \n",
    "\n",
    "1. Generar $T$ conjuntos de datos utilizando muestreo con reemplazo\n",
    "1. Entrenar un clasificador débil en cada conjunto\n",
    "1. Combinar los clasificadores débils mediante:\n",
    "\n",
    "$$\n",
    "H(x) = \\text{arg} \\max_{y \\in \\mathcal{Y}} \\sum_{t=1}^T \\mathbb{1} (h_t(x) = y)\n",
    "$$\n",
    "\n",
    "donde $\\mathbb{1}(\\cdot)$ es la función indicadora, que es 1 si su argumento es cierto y 0 en el caso contrario\n",
    "\n",
    "Lo anterior se conoce como voto por mayoría, ya que se escoge la etiqueta que fue mayormente votada por los clasificadores débiles\n",
    "\n",
    ":::{note}\n",
    "\n",
    "El procedimiento anterior es amigable con arquitecturas computacionales de múltiples procesadores pues cada entrenamiento es independiente de los demás\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplos oob**\n",
    "\n",
    "Cuando se utiliza muestreo con reemplazo la probabilidad de que el ejemplo $i$ sea incluido al menos una vez es 0.632 (se distribuye Poisson con $\\lambda=1$)\n",
    "\n",
    "Por lo tanto, por cada clasificador, hay un $36.8 \\%$ de ejemplos que no se ocupan. Estos ejemplos se denominan *out-of-bag* (oob)\n",
    "\n",
    "Una ventaja de bagging es que podemos utilizar los ejemplos oob de cada clasificador débil para medir el error de generalización\n",
    "\n",
    "**Clasificador inestable**\n",
    "\n",
    "Existe un traslape considerable entre los conjuntos remuestreados ($63.2 \\%$)\n",
    "\n",
    "Clasificador estable\n",
    ": Se refiere a un clasificador que es insensible a perturbaciones en el dataset\n",
    "\n",
    "En el caso de bagging, si los clasificadores débiles son estables, entonces su resultado será muy similar y por ende no habrá ganancia al construir un ensamble\n",
    "\n",
    ":::{important}\n",
    "\n",
    "Bagging es una técnica que promedia clasificadores débiles, es decir reduce varianza. Por ende funcionará mejor con clasificadores débiles de bajo sesgos (error) pero alta varianza\n",
    "\n",
    ":::\n",
    "\n",
    "En el caso de los árboles de decisión, mientras más profundo sea mayor inestabilidad se tiene. Adicionalmente, si los árboles no se podan son más inestables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "En bagging se realiza muestreo con reemplazo de los datos. [Random Forest](https://link.springer.com/article/10.1023/A:1010933404324) (RF) extiende esta idea realizando submuestreo de los atributos (features/características)\n",
    "\n",
    "En particular, cada split de cada clasificador débil (árbol) tiene acceso a un subconjunto aleatorio $M$ de los atributos originales. De esta forma se obtienen clasificadores débiles menos correlacionados (más independientes) y además más rápidos de entrenar\n",
    "\n",
    "La cantidad máxima de atributos por split es un hiperparámetro a calibrar. Típicamente se utiliza la raiz cuadrada del total de atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación en scikit-learn\n",
    "\n",
    "El módulo [`ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) de scikit-learn tiene implementaciones de *RandomForest* para problemas de clasificación y regresión. Nos enfocaremos en el primero\n",
    "\n",
    "Los principales argumentos de [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) son\n",
    "\n",
    "- `n_estimators`: Cantidad de clasificadores débiles (árboles)\n",
    "- `max_features`: Cantidad máxima de atributos por split\n",
    "- `bootstrap`: Booleano que indica si se realiza muestreo con reeplazo de los datos (por defecto `True`)\n",
    "- `n_jobs`: Número de nucleos de CPU\n",
    "- `class_weight`: Permite ponderar la importancia de las clases (útil para desbalance moderado)\n",
    "\n",
    "También recibe argumentos relacionados a los clasificadores débiles (árboles), entre ellos:\n",
    "\n",
    "- `criterion`:  Criterio para realizar la separación (split), `'entropy'` o `'gini'`\n",
    "- `max_depth`: Profundidad máxima de los árboles\n",
    "- `min_samples_split`: Número mínimo de muestras para permitir un `split`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(n_jobs=4),\n",
       "             param_grid={'criterion': ('entropy', 'gini'),\n",
       "                         'max_depth': [1, 5, 10, 20, None],\n",
       "                         'n_estimators': [1, 10, 20, 50, 100]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "params = {'criterion':('entropy', 'gini'),\n",
    "          'max_depth':[1, 5, 10, 20, None],\n",
    "          'n_estimators': [1, 10, 20, 50, 100]}\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=4)\n",
    "validator = GridSearchCV(model, params, cv=3, refit=True)\n",
    "validator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mejores modelos de acuerdo a la validación cruzada son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>gini</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.983933</td>\n",
       "      <td>0.011363</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>0.975997</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>gini</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>0.975997</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.975997</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.975997</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_criterion param_max_depth param_n_estimators  mean_test_score  \\\n",
       "41            gini              20                 10         0.983933   \n",
       "24         entropy            None                100         0.975997   \n",
       "44            gini              20                100         0.975997   \n",
       "39            gini              10                100         0.975997   \n",
       "37            gini              10                 20         0.975997   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "41        0.011363                1  \n",
       "24        0.019442                2  \n",
       "44        0.019442                2  \n",
       "39        0.019442                2  \n",
       "37        0.019442                2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"param_criterion\", \"param_max_depth\", \"param_n_estimators\", \n",
    "           \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "pd.DataFrame(validator.cv_results_)[columns].sort_values(by=\"rank_test_score\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor Random Forest es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 20, 'n_estimators': 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validator.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "A diferencia de GB, Random Forest prefiere árboles más profundos. Los árboles profundos tienen con bajo sesgo y alta varianza\n",
    "\n",
    ":::\n",
    "\n",
    "El resultado en el conjunto de test es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.95      0.98        22\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_rf =  validator.best_estimator_\n",
    "print(classification_report(y_test, best_rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de características embebida\n",
    "\n",
    "En cada split de cada árbol se obtiene una ganancia de información para el atributo seleccionado. Luego, para un atributo en particular, se puede obtener su ganancia de información promedio. \n",
    "\n",
    ":::{note}\n",
    "\n",
    "Un atributo que es más relevante (permite clasificar mejor), será escogido por una mayor cantidad de splits y por ende tendrá ganancia de información promedio mayor\n",
    "\n",
    ":::\n",
    "\n",
    "Se puede utilizar la ganancia de información promedio para hacer selección de características, sin embargo existe una limitación importante: El RF no detecta correlaciones entre atributos\n",
    "\n",
    ":::{warning}\n",
    "\n",
    "Si dos atributos están altamente correlacionados el RF no preferirá ninguna de ellas y la ganancia de información promedio de ambos será baja\n",
    "\n",
    ":::\n",
    "\n",
    "Para el ejemplo anterior se tiene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx_sort = np.argsort(best_rf.feature_importances_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6), tight_layout=True)\n",
    "ax.barh(np.array(X_names)[idx_sort], best_rf.feature_importances_[idx_sort], align='center')\n",
    "ax.set_xlabel('Ganancia de información\\npromedio (importancia)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
