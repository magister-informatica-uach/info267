{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensambles secuenciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting es una familia de algoritmos que buscan combinar estimadores (clasificadores o regresores) débiles de forma secuencial (cadena) para construir un estimador fuerte\n",
    "\n",
    "Estimador débil\n",
    ": Algoritmo que produce un resultado (al menos) levemente mejor que el azar \n",
    "\n",
    "Estimador fuerte\n",
    ": Algoritmo que produce un resultado correcto en la mayoría de los ejemplos\n",
    "\n",
    ":::{epigraph}\n",
    "\n",
    "Cualquier estimador débil puede ser mejorado (*boosted*) a un estimador fuerte\n",
    "\n",
    "-- [Robert Shapire](https://link.springer.com/article/10.1007/BF00116037)\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procedimiento general de un algoritmo de tipo boosting es:\n",
    "\n",
    "1. Entrenar un estimador débil con toda la distribución de datos\n",
    "1. Crear una nueva distribución que le da más peso a los errores del clasificador débil anterior\n",
    "1. Entrenar otro estimador débil en la nueva distribución\n",
    "1. Combinar los estimadores débiles y volver a 2\n",
    "\n",
    "Estos pasos se muestran esquemáticamente en la siguiente figura para el caso particular de clasificación:\n",
    "\n",
    "<img src=\"img/boosting.png\" width=\"700\">\n",
    "\n",
    ":::{important}\n",
    "\n",
    "El clasificador $H_2$ se encarga de corregir los errores de $H_1$. La combinación de $H_1$ y $H_2$ es el clasificador fuerte\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matemáticamente:\n",
    "\n",
    ":::{prf:algorithm} Algoritmo general de Boosting\n",
    "\n",
    "**Entradas** Un conjunto de datos $\\mathcal{D}$ y un número máximo de estimadores $T$\n",
    "\n",
    "1. Definir conjunto inicial $D_{1} = D$\n",
    "1. Para $t = 1, \\ldots, T$:\n",
    "    1. Entrenar un estimador débil sobre $D_t$\n",
    "    1. Evaluar el error del estimador débil\n",
    "    1. Ponderar los ejemplos en base al error para crear $D_{t+1}$\n",
    "1. Combinar las salidas de los estimadores débiles \n",
    "\n",
    ":::\n",
    "\n",
    "En esta lección utilizaremos árboles de decisión como estimador débil. Esto define los pasos de entrenamiento y evaluación del algoritmo anterior. Sólo resta definir como\n",
    "\n",
    "- Crear $D_{t+1}$\n",
    "- Combinar los estimadores débiles\n",
    "\n",
    "A continuación veremos como definen estos puntos dos algoritmos particulares de Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Boosting (Adaboost)\n",
    "\n",
    "[Adaboost](https://www.sciencedirect.com/science/article/pii/S002200009791504X) es un algoritmo diseñado para clasificación binaria $\\{-1,1\\}$ donde los clasificadores débiles se combinan linealmente como\n",
    "\n",
    "$$\n",
    "H_T(x) = \\sum_{t=1}^T \\alpha_t h_t(x)\n",
    "$$\n",
    "\n",
    "donde la clase predicha se obtiene aplicando la función signo sobre $H_T(x)$\n",
    "\n",
    "El ensamble se entrena minimizando la función de pérdida exponencial\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{L}(H_T) &= \\sum_{i=1}^N \\exp \\left (-y_i H_T(x_i) \\right) \\\\\n",
    "&= \\sum_{i=1}^N \\exp \\left (-y_i H_{T-1}(x_i) -y_i \\alpha_T h_T(x_i)\\right) \\\\\n",
    "&= \\sum_{i=1}^N w_i^{(T)}\\exp \\left (-y_i \\alpha_T h_T(x_i)\\right) \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Para entrenar el último clasificador de la secuencia $h_T$ podemos asumir $w_i^{(T)}$ constante\n",
    "\n",
    ":::\n",
    "\n",
    "Dividiendo la función de costo en los casos bien y mal clasificados\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{L}(H_T) &= \\sum_{i=1}^N w_i^{(T)}\\exp \\left (-y_i \\alpha_T h_T(x_i)\\right) \\\\\n",
    "&= \\sum_{h(x_i)y_i = 1} e^{-\\alpha_T} w_i^{(T)} + \\sum_{h(x_i)y_i \\neq 1} e^{\\alpha_T} w_i^{(T)} \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Derivando en función de $\\alpha$ se tiene\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2} \\log \\left(\\frac{1-\\epsilon_t}{\\epsilon_t} \\right),\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\n",
    "\\epsilon_t = \\frac{\\sum_{i=1}^N w_i^{(T)} \\mathbb{1}(h_t(x_i)\\neq y_i)}{\\sum_{i=1}^N w_i^{(T)}}\n",
    "$$\n",
    "\n",
    "donde $\\mathbb{1}()$ es la función indicadora que es 1 si su argumento es cierto o 0 en caso contrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto tenemos\n",
    "\n",
    ":::{prf:algorithm} Algoritmo Adaptive Boosting\n",
    "\n",
    "**Entradas** Un conjunto de datos $\\mathcal{D}$ y un número máximo de estimadores $T$\n",
    "\n",
    "1. Inicializar los pesos $w_i^{(1)} = 1/N$\n",
    "1. Para $t = 1, \\ldots, T$:\n",
    "    1. Entrenar un estimador débil $h_t$ sobre los datos ponderados por $w_i^{(t)}$\n",
    "    1. Calcular $\\alpha_t$\n",
    "    1. Actualizar los pesos $w_i^{(t+1)} = w_i^{(t)} \\exp (2 \\alpha_t \\mathbb{1}(h_t(x_i) \\neq y_i))$\n",
    "    \n",
    ":::\n",
    "\n",
    "El clasificador fuerte está totalmente definido por los clasificadores débiles $h_t$ y los ponderadores $\\alpha_t$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Los pesos de los datos se actualizan con los errores del último clasificador\n",
    "\n",
    ":::\n",
    "\n",
    ":::{important}\n",
    "\n",
    "Los ensambles de tipo boosting reducen progresivamente el sesgo (error) agregando secuencialmente clasificadores débiles\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "El algoritmo de [gradient boosting](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full) combina el método de gradiente descendente con el algoritmo general de boosting. Es capaz de hacer tanto clasificación como regresión y puede usar cualquier función de pérdida que sea derivable\n",
    "\n",
    "Sea una función de costo sobre un dataset $(x_i, y_i)_{i=1,\\ldots,N}$\n",
    "\n",
    "$$\n",
    "\\min \\sum_{i=1}^N L(y_i, H_T(x_i)),\n",
    "$$\n",
    "\n",
    "donde $H_T$ es el estimador fuerte\n",
    "\n",
    "Por ejemplo, en un problema de regresión, este se define como\n",
    "\n",
    "$$\n",
    "H_T(x_i) = \\sum_{t=1}^T h_t(x_i),\n",
    "$$\n",
    "\n",
    "es decir una suma de \"regresores\" débiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un problema de regresión se utiliza típicamente el error cuadrático medio como función de costo\n",
    "\n",
    "$$\n",
    "L(H_T(x_i), y_i) = \\frac{1}{2} \\left(y_i - H_T(x_i) \\right)^2\n",
    "$$\n",
    "\n",
    "Supongamos que tenemos $H_3 = h_1 + h_2 + h_3$ y deseamos agregar un nuevo estimador tal que\n",
    "\n",
    "$$\n",
    "H_4 = H_3 + h_4\n",
    "$$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Los estimadores se agregan de uno por uno de forma *greedy* \n",
    "\n",
    ":::\n",
    "\n",
    "Agregar el nuevo estimador debería acercar al estimador fuerte a la etiqueta es decir \n",
    "\n",
    "$$\n",
    "H_3 + h_4 \\approx y\n",
    "$$\n",
    "\n",
    "Para lograr esto el nuevo estimador $h_4$ se entrena **minimizando el residuo** $y-H_3$\n",
    "\n",
    "$$\n",
    "h_4 = \\text{arg} \\min_{h} \\sum_{i=1}^N L(h(x_i), y_i - H_3(x_i))\n",
    "$$\n",
    "\n",
    "Donde el residuo está relacionado a \n",
    "\n",
    "$$\n",
    "\\frac{dL(H_3(x_i), y_i)}{dH_3(x_i)} = H_3(x_i) - y_i\n",
    "$$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Si utilizamos el error cuadrático, entonces los residuos son equivalentes al negativo del gradiente, esta es la razón del nombre del algoritmo\n",
    "\n",
    ":::\n",
    "\n",
    "Para reducir el sobreajuste del ensamble (regularización) se agrega tipicamente\n",
    "\n",
    "$$\n",
    "H_{t+1} = H_t + \\nu h_t\n",
    "$$\n",
    "\n",
    "una constate $\\nu$ denominada tasa de aprendizaje. Esto disminuye la contribución de cada estimador débil (enlentece el entrenamiento)\n",
    "\n",
    ":::{note}\n",
    "\n",
    "En general una tasa de aprendizaje pequeña requerirá una mayor cantidad de clasificadores débiles. La ventaja de una tasa pequeña es que está relacionada a menores errores en el conjunto de test \n",
    "\n",
    ":::\n",
    "\n",
    "Para problemas de clasificación con $K$ clases se suele utilizar la siguiente función de costo, llamada generalmente *logarithmic loss* o *log loss*\n",
    "\n",
    "$$\n",
    "L(y_i, x_i) = \\sum_{k=1}^K y_{ik} \\log p_k (x_i)\n",
    "$$\n",
    "\n",
    "donde $y_i$ es un vector de largo K de tipo *one-hot* y $H(x)$ retorna también un vector de largo $K$ que se normaliza como\n",
    "\n",
    "$$\n",
    "p_k(x) = \\frac{e^{H^{(k)}(x)}}{ \\sum_{k=1}^K e^{H^{(k)}(x)} }\n",
    "$$\n",
    "\n",
    "tal que cada componente este en el rango $[0,1]$ y que además los $K$ sumen uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación en `scikit-learn`\n",
    "\n",
    "El módulo [`ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) de scikit-learn tiene implementaciones de *Gradient Boosting* para problemas de clasificación y regresión. Nos enfocaremos en la primera\n",
    "\n",
    "Los principales argumentos de [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) son\n",
    "\n",
    "- `loss`: La función de costo. Las opciones son `'log_loss'/'deviance'` (dependiendo de su versión de scikit-learn) o `'exponential'`\n",
    "- `n_estimators:` Cantidad de clasificadores débiles\n",
    "- `learning_rate`: Tasa de aprendizaje (no-negativo)\n",
    "- `subsample`: Booleano que indica si cada clasificador débil utiliza el dataset completo o una submuestra\n",
    "\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Si se utiliza `loss='exponential'` el algoritmo se vuelve equivalent a *AdaBoost* (sólo para clasificación binaria)\n",
    "\n",
    ":::\n",
    "\n",
    "También recibe argumentos relacionados a los clasificadores débiles (árboles), entre ellos:\n",
    "\n",
    "- `max_depth`: Profundidad máxima de los árboles\n",
    "- `min_samples_split`: Número mínimo de muestras para permitir un `split`\n",
    "\n",
    "El objeto tiene implementados los métodos usuales `fit`, `predict`, `predict_proba` y `decision_function`\n",
    "\n",
    "Adicionalmente cuenta con `staged_predict`, `staged_predict_proba` y `staged_decision_function`, que retornan las predicciones de los clasificadores débiles individuales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo** Entrenamiento de ensamble gradient boosting para clasificación de vino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_struct = load_wine()\n",
    "X = data_struct.data\n",
    "y = data_struct.target\n",
    "X_names = data_struct.feature_names\n",
    "\n",
    "np.random.seed(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza una validación cruzada buscando los mejores hiperparámetros del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': [0.1, 0.2, 0.5],\n",
       "                         'max_depth': [1, 5, 10, 20],\n",
       "                         'n_estimators': [1, 10, 20, 50, 100]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'learning_rate': [0.1, 0.2, 0.5], \n",
    "          'max_depth': [1, 5, 10, 20],\n",
    "          'n_estimators': [1, 10, 20, 50, 100]}\n",
    "\n",
    "model = GradientBoostingClassifier(loss='deviance')\n",
    "validator = GridSearchCV(model, params, cv=3, refit=True)\n",
    "validator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mejores modelos de acuerdo a la validación cruzada son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.959930</td>\n",
       "      <td>0.022174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.959930</td>\n",
       "      <td>0.022174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951994</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951994</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.944057</td>\n",
       "      <td>0.044622</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_learning_rate param_max_depth param_n_estimators  mean_test_score  \\\n",
       "24                 0.2               1                100         0.959930   \n",
       "41                 0.5               1                 10         0.959930   \n",
       "21                 0.2               1                 10         0.951994   \n",
       "42                 0.5               1                 20         0.951994   \n",
       "23                 0.2               1                 50         0.944057   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "24        0.022174                1  \n",
       "41        0.022174                1  \n",
       "21        0.033398                3  \n",
       "42        0.033398                3  \n",
       "23        0.044622                5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "columns = [\"param_learning_rate\", \"param_max_depth\", \"param_n_estimators\", \n",
    "           \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "pd.DataFrame(validator.cv_results_)[columns].sort_values(by=\"rank_test_score\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso hay varios modelos que obtuvieron el primer lugar en términos de *accuracy* promedio\n",
    "\n",
    "`GridSearchCV` retorna arbitrariamente el primero en orden de ejecución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 100}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validator.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "Boosting funciona bien con árboles poco profundos. Los árboles de poca profundidad suelen tener alto sesgo y baja varianza\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado de predicción en el conjunto de test es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       1.00      0.95      0.98        22\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_gb = validator.best_estimator_\n",
    "print(classification_report(y_test, best_gb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4), tight_layout=True)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, validator.predict(X_test), normalize=None,\n",
    "                                        ax=ax[0], cmap='Blues', colorbar=True);\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, validator.predict(X_test), normalize='true',\n",
    "                                        ax=ax[1], cmap='Blues', colorbar=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    "\n",
    "Otros algoritmos de Boosting con árboles de decisión extremadamente competitivos:\n",
    "\n",
    "- [HistGradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)\n",
    "- [XGBoost](http://dmlc.cs.washington.edu/xgboost.html)\n",
    "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "Estos implementan estrategias para mejorar la eficiencia y realizar cálculos paralelos/distribuidos \n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
