{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Deep Reinforced Learning*\n",
    "\n",
    "Hoy en día el estado del arte en el reconocimiento de patrones está dominado por las **redes neuronales profundas**\n",
    "\n",
    "- Visión Computacional: Redes Neuronales Convolucionales, Adversarios generativos\n",
    "- Reconocimiento de habla: Redes Recurrentes, WaveNet\n",
    "- Procesamiento de lenguaje Natural: Transformers\n",
    "\n",
    "Las redes neuronales son excelentes para representar el mundo: modelos\n",
    "\n",
    "> Podemos aprovechar esta capacidad para diseñar mejores algoritmos de aprendizaje reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuerdo: Value Learning\n",
    "\n",
    "En este tipo de algoritmos usamos un política de máxima utilidad para escoger acciones\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\text{arg} \\max_{a\\in \\mathcal{A}} Q(s, a)\n",
    "$$\n",
    "\n",
    "Y el problema entonces se reduce a aprender **Q**, *e.g* Q-Learning\n",
    "\n",
    "Sin embargo Q-learning tiene limitaciones: \n",
    "- requiere de heurísticas para explorar \n",
    "- espacio de acciones debe ser discreto\n",
    "- espacio de estados debe ser discreto\n",
    "\n",
    "De hecho el estado no puede ser demasiado grande, como veremos a continuación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiente: [Space invaders](https://es.wikipedia.org/wiki/Space_Invaders)\n",
    "\n",
    "Originalmente un juego de Arcade lanzando en 1978, en 1980 tuvo una versión para ATARI 2600\n",
    "\n",
    "> El objetivo es derrivar a los extraterrestres usando un cañon antes de que desciendan a la Tierra\n",
    "\n",
    "- El cañon puede moverse a la izquierda, a la derecha y disparar\n",
    "- Hay cuatro escudos con los que el cañon puede protegerse de los disparos enemigos\n",
    "- Mientras menos enemigos más rápido se mueven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "\n",
    "env = gym.make(\"SpaceInvaders-v0\") \n",
    "env.reset()\n",
    "end = False\n",
    "\n",
    "while not end:\n",
    "    a = env.action_space.sample()\n",
    "    s, r, end, info = env.step(a)\n",
    "    env.render() \n",
    "    sleep(.02)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espacio de estados\n",
    "display(env.observation_space)\n",
    "# Espacio de acciones\n",
    "display(env.action_space)\n",
    "display(env.action_space.n)\n",
    "display(env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo si usamos como estado un stack de 4 imágenes consecutivas y asumimos que cada pixel tiene 255 niveles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "255^{210\\cdot160\\cdot3\\cdot4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aproximación de funciones\n",
    "\n",
    "Claramente, el espacio de estados del ejemplo anterior es imposible de mantener en una tabla Q\n",
    "\n",
    "Ojo: este espacio de estados está aun lejos de un problema del mundo real\n",
    "\n",
    "> Nos va a faltar memoría para guardar la tabla y datos para poder entrenar nuestro agente\n",
    "\n",
    "¿Qué podemos hacer?\n",
    "\n",
    "> Usar una representación más compacta para Q\n",
    "\n",
    "En lugar de tener una tabla con todos las combinaciones estado/acción podemos\n",
    "\n",
    "> Aproximar **Q** usando un **modelo paramétrico**\n",
    "\n",
    "Esta es la idea principal tras *Value function approximation* (VFA) y *Q function approximation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El caso más sencillo es usar un **modelo lineal en sus parámetros**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat Q_\\theta(s,a) &= \\theta_0 \\phi_0(s, a) + \\theta_1 \\phi_1(s, a) + \\theta_2 \\phi_2(s, a) + \\ldots + \\theta_M \\phi_M (s,a) \\nonumber \\\\\n",
    "&= \\sum_{j=0}^M \\theta_j \\phi_j (s,a) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde \n",
    "- $\\{ \\theta\\}$ es un vector de parámetros con $M+1$ componentes\n",
    "- $\\{\\phi\\}$ es un conjunto de funciones base, *e.g.* polinomios, Fourier, árbol de decisión, kernels\n",
    "\n",
    "En lugar de aprender $Q$ explicitamente el objetivo es aprender $\\theta$\n",
    "\n",
    "> La cantidad de parámetros es ahora independiente de la dimensionalidad del estado\n",
    "\n",
    "\n",
    "\n",
    "Al igual que antes nuestro objetivo es acercanos a la solución de la Ecuación de Bellman\n",
    "\n",
    "Podemos escribir esto como el siguiente problema de optimización\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\| R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} \\hat Q_\\theta( s',a') - \\hat Q_\\theta(s,a)\\|^2\n",
    "$$\n",
    "\n",
    "de donde podemos aprender $\\theta$ iterativamente usando usando gradiente descendente \n",
    "\n",
    "$$\n",
    "\\theta_j \\leftarrow \\theta_j + 2 \\alpha \\left(R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} \\hat Q_\\theta (s',a') - \\hat Q_\\theta(s,a) \\right) \\phi_j(s,a)\n",
    "$$\n",
    "\n",
    "Sin embargo, un modelo lineal podría ser muy limitado\n",
    "\n",
    "En la unidad 1 estudiamos el estado del arte en aproximación de funciones: **redes neuronales artificiales**\n",
    "\n",
    "> A continuación veremos como usar redes profundas para aproximar la función Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) \n",
    "\n",
    "> En [(Minh et al. 2013)](https://arxiv.org/abs/1312.5602) se usaron redes neuronales profundas de tipo convolucional para resolver una serie de juegos de ATARI con Aprendizaje Reforzardo obteniendo [desempeño sobre-humano en muchos de las pruebas](https://deepmind.com/blog/article/deep-reinforcement-learning). El modelo, llamado *Deep Q-network*, utiliza como estado el valor de todos los píxeles de cuatro cuadros consecutivos.\n",
    "\n",
    "La idea clave es\n",
    "\n",
    "> Aprovechar la capacidad de las redes neuronales profundas para representar datos complejos, e.g. imágenes \n",
    "\n",
    "o más en concreto\n",
    "\n",
    "> Aproximar la función Q usando una red convolucional entrenada directamente sobre los píxeles\n",
    "\n",
    "Veremos primero una formulación general y luego su aplicación al caso de imágenes\n",
    "\n",
    "### Modelo\n",
    "\n",
    "- La entrada de la red es el estado $s$. \n",
    "    - El vector de estado puede tener valores continuos o discretos\n",
    "- La salida de la red neuronal son los valores $Q(s, a_1), Q(s, a_2), \\ldots, Q(s, a_N)$\n",
    "    - Se considera un espacio de acciones discreto\n",
    "    - Esto es más eficiente que considerar $a'$ como una entrada y retornar $Q(s,a')$\n",
    "- La cantidad y tipo de las capas intermedias es decisión del usuario\n",
    "    - Si tenemos datos continuos (atributos) usamos capas completamente conectadas\n",
    "    - Si usamos píxeles es natural usar capas convolucionales\n",
    "- La función de perdida que se ocupa en DQN es el error cuadrático medio entre la ecuación de Bellman y la predicción de la red\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}\\left[\\left \\| R(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}} Q_\\theta(s', a') - Q_\\theta(s, a)\\right \\|^2\\right]\n",
    "$$\n",
    "\n",
    "Recordemos: $s'$ es el estado al que llegamos luego de ejecutar $a$ sobre $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN con estados continuos: El retorno del carro con péndulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previamente fue necesario discretizar el estado para construir la matriz Q\n",
    "\n",
    "En DQN podemos obviar este paso y sus complicaciones\n",
    "\n",
    "Vamos a usar una red neuronal con 3 capas completamente conectadas, 4 entradas y 2 salidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MultilayerPerceptron(torch.nn.Module):    \n",
    "    def __init__(self, n_input, n_output, n_hidden=10):\n",
    "        super(type(self), self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.output = torch.nn.Linear(n_hidden, n_output)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.hidden1(x))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return  self.output(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay y Memory replay\n",
    "\n",
    "El entrenamiento usando sólo una instancia es bastante ruidoso (y un poco lento)\n",
    "\n",
    "Consideremos también que estamos entrenando con muestras muy correlacionadas (no iid)\n",
    "\n",
    "Sabemos que esto puede introducir sesgos el entrenamiento de una red neuronal\n",
    "\n",
    "> Para entrenar la DQN adecuadamente  (Minh et al 2013) propone un astuto \"truco\" llamado *Experience replay*  \n",
    "\n",
    "Esto consiste en almacenar la historia del agente en una memoria: *replay memory*\n",
    "\n",
    "Cada elemento en la memoria es una tupla $(s_t, a_t, r_{t+1}, s_{t+1})$\n",
    "\n",
    "Con esto se crean mini-batches en orden aleatorio para entrenar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, state_dim, memory_length=2000): \n",
    "        self.length = memory_length\n",
    "        self.pointer = 0\n",
    "        self.filled = False\n",
    "        # Tensores vacíos para la historia\n",
    "        self.s_current = torch.zeros((memory_length,) + state_dim)\n",
    "        self.s_future = torch.zeros((memory_length,) + state_dim)\n",
    "        self.a = torch.zeros(memory_length, 1, dtype=int)\n",
    "        self.r = torch.zeros(memory_length, 1)\n",
    "        # Adicionalmente guardaremos la condición de término\n",
    "        self.end = torch.zeros(memory_length, 1, dtype=bool)\n",
    "    \n",
    "    def push(self, s_current, s_future, a, r, end):\n",
    "        # Agregamos una tupla en la memoria\n",
    "        self.s_current[self.pointer] = s_current\n",
    "        self.s_future[self.pointer] = s_future\n",
    "        self.a[self.pointer] = a\n",
    "        self.r[self.pointer] = r \n",
    "        self.end[self.pointer] = end\n",
    "        if self.pointer + 1 == self.length:\n",
    "            self.filled = True\n",
    "        self.pointer =  (self.pointer + 1) % self.length\n",
    "        \n",
    "    def sample(self, size=64):        \n",
    "        # Extraemos una muestra aleatoria de la memoria\n",
    "        if self.filled:\n",
    "            idx = np.random.choice(self.length, size)\n",
    "        elif self.pointer > size:\n",
    "            idx = np.random.choice(self.pointer, size)\n",
    "        else:\n",
    "            return None        \n",
    "        return self.s_current[idx], self.s_future[idx], self.a[idx], self.r[idx], self.end[idx]    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Double DQN*\n",
    "\n",
    "\n",
    "En [(van Hasselt, Guer y Silver, 2015)](https://arxiv.org/pdf/1509.06461.pdf) los autores notaron un problema importante en DQN. \n",
    "\n",
    "> Cuando calculamos $Q(s, a)$ y $\\max Q(s', a')$ usando la misma red es muy posible que sobre-estimemos la calidad de nuestro objetivo. Adicionalmente si el objetivo cambia constantemente el entrenamiento será inestable\n",
    "\n",
    "\n",
    "La solución propuesta consiste en usar redes neuronales distintas para la escoger la acción y para calcular el objetivo (ecuación de Bellman)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi) = \\mathbb{E}\\left[\\left \\| R(s,a) + \\gamma Q_\\phi\\left(s', \\text{arg}\\max_{a' \\in \\mathcal{A}} Q_\\theta(s', a')\\right) - Q_\\theta(s, a)\\right\\|^2\\right]\n",
    "$$\n",
    "\n",
    "Usamos $Q_\\theta$ con parámetros $\\theta$ para escoger la acción: *policy network*\n",
    "\n",
    "Usamos $Q_\\phi$ con parámetros $\\phi$ para construir el objetivo: *target network*\n",
    "\n",
    "- Ambas redes comparten arquitectura y cantidad de neuronas\n",
    "- Sólo se optimiza la *policy network*\n",
    "- Después de un cierto número de épocas los parametros de la policy network \"se copian\" en la *target network*\n",
    "\n",
    "¿Cada cuantas épocas actualizo la *target network*? Otro hyperparámetro para el algoritmo... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras consideraciones\n",
    "\n",
    "- El optimizador a usar es ADAM: Gradiente descendente con tasa de aprendizaje adaptiva\n",
    "- Se usa la heurística $\\epsilon$ greedy para favorecer la exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "    \n",
    "class DeepQNetwork:\n",
    "    def __init__(self, q_model, gamma=0.99, double_dqn=False, learning_rate=1e-3, \n",
    "                 target_update_freq=500, clip_grads=True, clip_error=True):\n",
    "        self.double_dqn = double_dqn\n",
    "        self.gamma = gamma\n",
    "        self.q_policy = q_model\n",
    "        self.n_output = q_model.output.out_features\n",
    "        self.clip_error = clip_error\n",
    "        self.clip_grads = clip_grads\n",
    "        self.update_counter = 0\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.RMSprop(self.q_policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        if double_dqn:\n",
    "            self.q_target = copy.deepcopy(self.q_policy)\n",
    "            self.q_target.eval()\n",
    "    \n",
    "    def select_action(self, state, epsilon=0.):\n",
    "        # Estrategia epsilon greedy para seleccionar acción\n",
    "        if torch.rand(1).item() < 1. - epsilon: \n",
    "            self.q_policy.eval()\n",
    "            with torch.no_grad():\n",
    "                q = self.q_policy(state)[0]\n",
    "                a = q.argmax().item()\n",
    "                q = q[a]\n",
    "            self.q_policy.train()\n",
    "        else:\n",
    "            q = None\n",
    "            a = torch.randint(high=self.n_output, size=(1,)).item() \n",
    "        \n",
    "        return a, q\n",
    "    \n",
    "    def update(self, mini_batch):\n",
    "        self.update_counter += 1\n",
    "        state, state_next, action, reward, end = mini_batch\n",
    "        # Calcular Q\n",
    "        q_current = self.q_policy(state).gather(1, action)\n",
    "        with torch.no_grad():\n",
    "            if not self.double_dqn:\n",
    "                q_next_best = self.q_policy(state_next).max(1, keepdim=True)[0]\n",
    "            else:\n",
    "                action_next = self.q_policy(state_next).argmax(dim=1, keepdim=True)\n",
    "                q_next_best = self.q_target(state_next).gather(1, action_next)               \n",
    "        # Construir el target: r + gamma*max Q(s', a')\n",
    "        td_target = reward\n",
    "        td_target[~end] += self.gamma*q_next_best[~end]\n",
    "        td_target[end] = -1.\n",
    "        # Calcular pérdido y sus gradientes\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(q_current, td_target)\n",
    "        if self.clip_error:\n",
    "            loss.clamp_(-1., 1.)\n",
    "        loss.backward()\n",
    "        # Cortar gradientes grandes (mejora la estabilidad)\n",
    "        if self.clip_grads:\n",
    "            for param in self.q_policy.parameters():\n",
    "                param.grad.data.clamp_(-1., 1.)\n",
    "            # torch.nn.utils.clip_grad.clip_grad_norm_(self.model.parameters(), 10)\n",
    "        # Actualizar\n",
    "        self.optimizer.step()\n",
    "        # Transfer policy to target\n",
    "        self.transfer_policy2target()\n",
    "        # Retornar el valor de la loss\n",
    "        return loss.item()\n",
    "    \n",
    "    def transfer_policy2target(self):\n",
    "        if self.double_dqn:            \n",
    "            if self.update_counter % self.target_update_freq == 0:\n",
    "                self.q_target.load_state_dict(self.q_policy.state_dict())\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "\n",
    "fig, ax = plt.subplots(4, figsize=(6, 5), sharex=True, tight_layout=True)\n",
    "\n",
    "def smooth_data(x, window_length=10):\n",
    "    return convolve(x, np.ones(window_length)/window_length, mode='valid')\n",
    "\n",
    "def update_plot(step, episode, smooth_window=10, target=195, target_update=500):\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "    episodes = np.arange((episode))\n",
    "    ax[0].scatter(episodes, diagnostics['rewards'], s=1)      \n",
    "    if episode > smooth_window:\n",
    "        ax[0].plot(episodes[:-smooth_window+1], \n",
    "                   smooth_data(diagnostics['rewards']), alpha=0.5, lw=2)        \n",
    "    ax[1].plot(episodes, diagnostics['loss'])\n",
    "    ax[2].plot(episodes, np.array(diagnostics['q_sum'])/(np.array(diagnostics['q_N'])+1e-4))\n",
    "    #for update_time in diagnostics['target_updates']:\n",
    "    #    ax[1].plot([update_time, update_time], \n",
    "    #               [diagnostics['loss'].min(), diagnostics['loss'].max()], 'k--', alpha=0.1)                  \n",
    "                   \n",
    "    ax[0].plot(episodes, [target]*len(episodes), 'k--')\n",
    "    ax[0].set_ylabel('Recompensa');\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[2].set_ylabel('Q promedio')\n",
    "    ax[3].plot(episodes, epsilon(episodes))\n",
    "    ax[3].set_ylabel('Epsilon')\n",
    "    ax[3].set_xlabel('Episodios')\n",
    "    ax[0].set_title(\"Paso %d\" % (step))\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos\n",
    "\n",
    "Usemos la clase DeepQNetwork para entrenar una agente que resuelva el problema del carro con péndulo\n",
    "\n",
    "- Si uso `memory_length=1` es equivalente a no usar *replay memory*. Mientras más corta es la memoria más ruidoso es el entrenamiento. \n",
    "- Con el parámetro `double_dqn` podemos activar la red objetivo que se actualizará cada `target_update_freq` pasos\n",
    "- Otros parámetros a considerar son `gamma`, el tamaño del batch, la tasa de aprendizaje y la arquitectura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "n_state = (env.observation_space.shape[0],) # Número de estados\n",
    "n_action = env.action_space.n # Número de acciones\n",
    "\n",
    "dqn_model = DeepQNetwork(q_model=MultilayerPerceptron(n_state[0], n_action, n_hidden=16),\n",
    "                         gamma = 0.99,\n",
    "                         double_dqn=True,\n",
    "                         target_update_freq=100,\n",
    "                         learning_rate=1e-4)\n",
    "\n",
    "def epsilon(episode, epsilon_init=1., epsilon_end=0.1, epsilon_rate=1e-2):\n",
    "    return epsilon_end + (epsilon_init - epsilon_end) * np.exp(-epsilon_rate*episode) \n",
    "\n",
    "memory = ReplayMemory(n_state, memory_length=100000)        \n",
    "\n",
    "diagnostics = {'rewards': [0], 'loss': [0],\n",
    "               'q_sum': [0], 'q_N': [0]}\n",
    "\n",
    "episode = 1\n",
    "end = False\n",
    "state = env.reset()\n",
    "for step in tqdm(range(30000)):    \n",
    "    # Escoger acción\n",
    "    state = torch.tensor(state).float()\n",
    "    a, q = dqn_model.select_action(state.unsqueeze(0), epsilon(episode))\n",
    "    if q is not None:\n",
    "        diagnostics['q_sum'][-1] += q\n",
    "        diagnostics['q_N'][-1] += 1\n",
    "    # Ejecutar acción\n",
    "    s, r, end, info = env.step(a)        \n",
    "    # Guardar en memoria\n",
    "    memory.push(state, torch.tensor(s).float(), a, torch.tensor(r/200.), end)\n",
    "    # Actualizar modelo    \n",
    "    mini_batch = memory.sample(32)\n",
    "    if not mini_batch is None:\n",
    "        diagnostics['loss'][-1] += dqn_model.update(mini_batch)            \n",
    "    # Guardar nuevo estado y continuar\n",
    "    diagnostics['rewards'][-1] += r\n",
    "    state = s \n",
    "    # Preparar siguiente episodio\n",
    "    if end:\n",
    "        if episode % 25 == 0:\n",
    "            update_plot(step, episode)\n",
    "        episode += 1  \n",
    "        end = False\n",
    "        state = env.reset()\n",
    "        diagnostics['rewards'].append(0)\n",
    "        diagnostics['loss'].append(0)\n",
    "        diagnostics['q_sum'].append(0)\n",
    "        diagnostics['q_N'].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuición\n",
    "\n",
    "> La *loss* está subiendo, mi algoritmo está mal\n",
    "\n",
    "No necesariamente, recordemos que el \"training set\" está cambiando con cada paso\n",
    "\n",
    "Es normal ver que los *loss* tenga subidas abruptas \n",
    "\n",
    "En cierto modo estas subidas son deseables pues significa que nuestro algoritmo está \"sorprendiendose\" con nuevos estados\n",
    "\n",
    "> La recompensa promedio se fue en picada, mi algoritmo está mal\n",
    "\n",
    "Lo que está ocurriendo es un problema común en deep RL que se llama: **\"olvido catastrófico\"** \n",
    "\n",
    "El modelo es cada vez más bueno resolviendo el problema y la memoria se llena de \"buenas transiciones\"\n",
    "\n",
    "Los *minibatches* ya no tienen ejemplos erroneos: El modelo ya no sabe lo que está mal \n",
    "\n",
    "Se puede combatir con *prioritized replay memory* o con otros heurísticas\n",
    "\n",
    "- Reservar un espacio en la memoria para las transiciones (de exploración) iniciales \n",
    "- **Early stopping:** Guardar el modelo de mayor recompensa y detener el aprendizaje si la recompensa no sube en un cierto número de episodios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probando el agente DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "for k in range(500):\n",
    "    env.render()\n",
    "    state = torch.from_numpy(np.array(env.state).astype('float32')).unsqueeze(0)\n",
    "    a, q = dqn_model.select_action(state)\n",
    "    s_future, r, end, info = env.step(a)\n",
    "    if r == 0:\n",
    "        display(k)\n",
    "        break\n",
    "display(end, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Rainbow DQN](https://arxiv.org/pdf/1710.02298.pdf)\n",
    "\n",
    "Rainbow DQN es un trabajo que reune todos los avances relacionados a DQN\n",
    "\n",
    "1. *Double DQN*: Usar un modelo separado para calcular los targets\n",
    "1. *Replay memory*: Almacenar experiencias pasadas y muestrear aleatoriamente de la memoria para disminuir la correlación \n",
    "1. [*Prioritized experience replay*](https://arxiv.org/abs/1511.05952): El minibatch se construye ponderando las experiencias anteriores en función del error. Experiencias con más error tienen más probabilidad de ser escogidas. \"Se puede aprender más de los ejemplos más difíciles\"\n",
    "1. [*Dueling networks*](https://arxiv.org/abs/1511.06581): Se separa la función $Q(s,a) = V(s) + A(s,a)$, donde cada función es una red neuronal que comparten capas. Con esto se puede aprender el valor de cada estado de forma desacoplada a la importancia relativa de las acciones\n",
    "1. [*Noisy nets*](https://arxiv.org/abs/1706.10295): Capas de redes neuronales que incluyen un ruido paramétrico en sus parámetros lo que permite explorar sin necesidad de la heurística $\\epsilon$-greddy\n",
    "1. entre otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
