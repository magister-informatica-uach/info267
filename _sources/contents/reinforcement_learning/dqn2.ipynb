{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN a partir de píxeles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El *gym* de OpenAI tiene una colección de juegos de ATARI 2600 que pueden usarse como benchmark\n",
    "\n",
    "La mayoría tiene una versión normal y una versión RAM\n",
    "\n",
    "- En la versión normal las observaciones son imágenes de 260x120x3\n",
    "- En la versión RAM las observaciones son 128 bits que corresponden a la memoria de la consola\n",
    "\n",
    "En este ejemplo nos concentraremos en la versión normal y usaremos redes convolucionales para resolverlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"PongNoFrameskip-v4\"\n",
    "#env_name =\"BreakoutNoFrameskip-v4\"\n",
    "#env_name = \"SpaceInvadersNoFrameskip-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "\n",
    "display(\"Las acciones de este ambiente:\", env.unwrapped.get_action_meanings())\n",
    "\n",
    "display(\"La dimensión del estado:\", state.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 4), tight_layout=True)\n",
    "ax.axis('off')\n",
    "ax.imshow(state);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estado es una imagen de 210 x 160 x 3 pixeles\n",
    "\n",
    "Para facilitar el entrenamiento se recomienda hacer un preprocesamiento como el que sigue\n",
    "\n",
    "1. (opcional) Descartar parte de la imagen que no aporta información\n",
    "1. Reescalar la imagen a un menor tamaño\n",
    "1. Combinar los canales y generar una imagen de escala de grises\n",
    "1. Crear un stack de cuatro frames como representación del estado\n",
    "1. Convertir los pixeles a float y normalizar al rango [0, 1]\n",
    "\n",
    "Para esto usaremos la librería *torchvision* y los *wrappers* de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torchvision\n",
    "from atari_wrappers import NoopResetEnv, FireResetEnv, EpisodicLifeEnv, WarpFrame, ClipRewardEnv, FrameStack, MaxAndSkipEnv\n",
    "\n",
    "def wrap_env(env, skip=4, k=4):\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=skip) # Descarta una cierta cantidad de cuadros, toma el máximo entre cuadros sucesivos y aplica la misma acción en todos los cuadros\n",
    "    env = EpisodicLifeEnv(env) # Perder una vida es equivalente a perder el episodio\n",
    "    env = FireResetEnv(env) # Hace un disparo al inicio (algunos juegos lo necesitan)\n",
    "    env = WarpFrame(env) # Crop, resize y conversión a escala de  grises\n",
    "    env = ClipRewardEnv(env) # La recompensa se corta en [-1, 1] (mejora la estabilidad)\n",
    "    env = FrameStack(env, k=k) # El ambiente entrega de a 4 frames    \n",
    "    return env\n",
    "\n",
    "\n",
    "env = wrap_env(gym.make(env_name))\n",
    "env.reset()\n",
    "state, _, _, _ = env.step(2)\n",
    "\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# Función de para preprocesar\n",
    "def preprocess(states):\n",
    "    tmp = []\n",
    "    for k in range(states.count()):\n",
    "        tmp.append(transforms(states.frame(k)))\n",
    "    return torch.cat(tmp) # Esto es un tensor de 4x84x84\n",
    "\n",
    "transformed_state = preprocess(state)\n",
    "\n",
    "display(\"Tamaño del tensor transformado:\", transformed_state.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(8, 2), tight_layout=True)\n",
    "for k in range(4):\n",
    "    ax[k].matshow(transformed_state[k, :, :].numpy(), cmap=plt.cm.Greys_r);\n",
    "    ax[k].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red convolucional para estimar la función Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(torch.nn.Module):    \n",
    "    def __init__(self, n_input, n_output, n_filters=32, n_hidden=256):\n",
    "        super(type(self), self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(n_input, n_filters, kernel_size=8, stride=4)\n",
    "        self.conv2 = torch.nn.Conv2d(n_filters, n_filters, kernel_size=4, stride=2)\n",
    "        self.conv3 = torch.nn.Conv2d(n_filters, n_filters, kernel_size=3, stride=1)\n",
    "        self.linear1 = torch.nn.Linear(7 * 7 * n_filters, n_hidden)\n",
    "        self.output = torch.nn.Linear(n_hidden, n_output)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.conv1(x))\n",
    "        h = self.activation(self.conv2(h))\n",
    "        h = self.activation(self.conv3(h))\n",
    "        h = h.view(-1, 7*7*32)\n",
    "        h = self.activation(self.linear1(h))\n",
    "        return  self.output(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, state_dim, memory_length=2000): \n",
    "        self.length = memory_length\n",
    "        self.pointer = 0\n",
    "        self.filled = False\n",
    "        # Tensores vacíos para la historia\n",
    "        self.s_current = torch.zeros((memory_length,) + state_dim)\n",
    "        self.s_future = torch.zeros((memory_length,) + state_dim)\n",
    "        self.a = torch.zeros(memory_length, 1, dtype=int)\n",
    "        self.r = torch.zeros(memory_length, 1)\n",
    "        # Adicionalmente guardaremos la condición de término\n",
    "        self.end = torch.zeros(memory_length, 1, dtype=bool)\n",
    "    \n",
    "    def push(self, s_current, s_future, a, r, end):\n",
    "        # Agregamos una tupla en la memoria\n",
    "        self.s_current[self.pointer] = s_current\n",
    "        self.s_future[self.pointer] = s_future\n",
    "        self.a[self.pointer] = a\n",
    "        self.r[self.pointer] = r \n",
    "        self.end[self.pointer] = end\n",
    "        if self.pointer + 1 == self.length:\n",
    "            self.filled = True\n",
    "        self.pointer =  (self.pointer + 1) % self.length\n",
    "        \n",
    "    def sample(self, size=64):        \n",
    "        # Extraemos una muestra aleatoria de la memoria\n",
    "        if self.filled:\n",
    "            idx = np.random.choice(self.length, size)\n",
    "        elif self.pointer > size:\n",
    "            idx = np.random.choice(self.pointer, size)\n",
    "        else:\n",
    "            return None        \n",
    "        return self.s_current[idx], self.s_future[idx], self.a[idx], self.r[idx], self.end[idx]    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente DQN\n",
    "\n",
    "Detalles prácticos a considerar\n",
    "\n",
    "- **Huber Loss:** Combinación del error cuadrático y el error absoluto. Se usa el error cuadrático para errores pequeños y el error absoluto para errores grandes. Mejora la estabilidad numérica del algoritmo\n",
    "- **Gradient clipping:** Limitar el gradiente en un rango [mínimo, máximo], usualmente $[0,1]$. También se puede limitar dividiendo por la norma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "    \n",
    "class DeepQNetwork:\n",
    "    def __init__(self, q_model, gamma=0.99, double_dqn=False, learning_rate=1e-3, \n",
    "                 target_update_freq=500, clip_grads=True, clip_error=False, huber=False):\n",
    "        self.double_dqn = double_dqn\n",
    "        self.gamma = gamma\n",
    "        self.q_policy = q_model\n",
    "        self.n_output = q_model.output.out_features\n",
    "        self.clip_error = clip_error\n",
    "        self.clip_grads = clip_grads\n",
    "        self.update_counter = 0\n",
    "        self.target_update_freq = target_update_freq\n",
    "        if not huber:\n",
    "            self.criterion = torch.nn.MSELoss()\n",
    "        else:\n",
    "            self.criterion = torch.nn.SmoothL1Loss()\n",
    "        self.optimizer = torch.optim.RMSprop(self.q_policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        if double_dqn:\n",
    "            self.q_target = copy.deepcopy(self.q_policy)\n",
    "            self.q_target.eval()\n",
    "    \n",
    "    def select_action(self, state, epsilon=0.):\n",
    "        # Estrategia epsilon greedy para seleccionar acción\n",
    "        if torch.rand(1).item() < 1. - epsilon: \n",
    "            self.q_policy.eval()\n",
    "            with torch.no_grad():\n",
    "                q = self.q_policy(state)[0]\n",
    "                a = q.argmax().item()\n",
    "                q = q[a]\n",
    "            self.q_policy.train()\n",
    "        else:\n",
    "            q = None\n",
    "            a = torch.randint(high=self.n_output, size=(1,)).item() \n",
    "        \n",
    "        return a, q\n",
    "    \n",
    "    def update(self, mini_batch):\n",
    "        self.update_counter += 1\n",
    "        state, state_next, action, reward, end = mini_batch\n",
    "        # Calcular Q\n",
    "        q_current = self.q_policy(state).gather(1, action)\n",
    "        with torch.no_grad():\n",
    "            if not self.double_dqn:\n",
    "                q_next_best = self.q_policy(state_next).max(1, keepdim=True)[0]\n",
    "            else:\n",
    "                action_next = self.q_policy(state_next).argmax(dim=1, keepdim=True)\n",
    "                q_next_best = self.q_target(state_next).gather(1, action_next)               \n",
    "        # Construir el target: r + gamma*max Q(s', a')\n",
    "        td_target = reward\n",
    "        td_target[~end] += self.gamma*q_next_best[~end]\n",
    "        td_target[end] = -1.\n",
    "        # Calcular pérdido y sus gradientes\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(q_current, td_target)\n",
    "        if self.clip_error:\n",
    "            loss.clamp_(-1., 1.)\n",
    "        loss.backward()\n",
    "        # Cortar gradientes grandes (mejora la estabilidad)\n",
    "        if self.clip_grads:\n",
    "            for param in self.q_policy.parameters():\n",
    "                param.grad.data.clamp_(-1., 1.)\n",
    "            #torch.nn.utils.clip_grad.clip_grad_norm_(self.q_policy.parameters(), 10)\n",
    "        # Actualizar\n",
    "        self.optimizer.step()\n",
    "        # Transfer policy to target\n",
    "        self.transfer_policy2target()\n",
    "        # Retornar el valor de la loss\n",
    "        return loss.item()\n",
    "    \n",
    "    def transfer_policy2target(self):\n",
    "        if self.double_dqn:            \n",
    "            if self.update_counter % self.target_update_freq == 0:\n",
    "                self.q_target.load_state_dict(self.q_policy.state_dict())\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficas de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "\n",
    "fig, ax = plt.subplots(4, figsize=(6, 5), sharex=True, tight_layout=True)\n",
    "\n",
    "def smooth_data(x, window_length=10):\n",
    "    return convolve(x, np.ones(window_length)/window_length, mode='valid')\n",
    "\n",
    "def update_plot(step, episode, smooth_window=10, target=None):\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "    episodes = np.arange((episode))\n",
    "    ax[0].scatter(episodes, diagnostics['rewards'], s=1)      \n",
    "    if episode > smooth_window:\n",
    "        ax[0].plot(episodes[:-smooth_window+1], \n",
    "                   smooth_data(diagnostics['rewards']), alpha=0.5, lw=2)        \n",
    "    ax[1].plot(episodes, diagnostics['loss'])\n",
    "    ax[2].plot(episodes, np.array(diagnostics['q_sum'])/(np.array(diagnostics['q_N'])+1e-4))\n",
    "    if not target is None:               \n",
    "        ax[0].plot(episodes, [target]*len(episodes), 'k--')\n",
    "    ax[0].set_ylabel('Recompensa');\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[2].set_ylabel('Q promedio')\n",
    "    ax[3].plot(episodes, epsilon(episodes))\n",
    "    ax[3].set_ylabel('Epsilon')\n",
    "    ax[3].set_xlabel('Episodios')\n",
    "    ax[0].set_title(\"Paso %d\" % (step))\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "env = wrap_env(gym.make(env_name))\n",
    "n_state = (4, 84, 84)\n",
    "n_action = env.action_space.n \n",
    "\n",
    "dqn_model = DeepQNetwork(q_model=ConvolutionalNeuralNetwork(n_state[0], n_action),\n",
    "                         gamma = 0.99,\n",
    "                         double_dqn=True,\n",
    "                         target_update_freq=1000,\n",
    "                         learning_rate=1e-4, huber=True)\n",
    "\n",
    "def epsilon(episode, epsilon_init=0.1, epsilon_end=0.01, epsilon_rate=1e-2):\n",
    "    return epsilon_end + (epsilon_init - epsilon_end) * np.exp(-epsilon_rate*episode) \n",
    "\n",
    "memory = ReplayMemory(n_state, memory_length=10000)        \n",
    "\n",
    "diagnostics = {'rewards': [0], 'loss': [0],\n",
    "               'q_sum': [0], 'q_N': [0]}\n",
    "\n",
    "episode = 1\n",
    "end = False\n",
    "stacked_states = env.reset()\n",
    "\n",
    "for step in tqdm(range(100000)):    \n",
    "    # Escoger acción\n",
    "    state = preprocess(stacked_states)\n",
    "    a, q = dqn_model.select_action(state.unsqueeze(0), epsilon(episode))\n",
    "    if q is not None:\n",
    "        diagnostics['q_sum'][-1] += q\n",
    "        diagnostics['q_N'][-1] += 1\n",
    "    \n",
    "    # Aplicar la acción \n",
    "    stacked_states_next, r, end, info = env.step(a)  \n",
    "    diagnostics['rewards'][-1] += r\n",
    "    # Guardar en memoria\n",
    "    memory.push(state, preprocess(stacked_states_next), \n",
    "                a, torch.tensor(r), end)\n",
    "    \n",
    "    stacked_states = stacked_states_next\n",
    "    \n",
    "    # Actualizar modelo    \n",
    "    mini_batch = memory.sample(32)\n",
    "    if not mini_batch is None:\n",
    "        diagnostics['loss'][-1] += dqn_model.update(mini_batch)            \n",
    "    \n",
    "    # Preparar siguiente episodio\n",
    "    if end:\n",
    "        if episode % 5 == 0:\n",
    "            update_plot(step, episode)\n",
    "        episode += 1   \n",
    "        end = False\n",
    "        stacked_states = env.reset()\n",
    "        diagnostics['rewards'].append(0)\n",
    "        diagnostics['loss'].append(0)\n",
    "        diagnostics['q_sum'].append(0)\n",
    "        diagnostics['q_N'].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el agente para futura referencia y evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"mi_modelo.pkl\", \"wb\") as f:\n",
    "    pickle.dump([dqn_model, dqn_model.q_policy.state_dict()], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "\n",
    "env = wrap_env(gym.make(env_name), skip=1)\n",
    "stacked_states = env.reset()\n",
    "end = False\n",
    "\n",
    "while not end:\n",
    "    state = preprocess(stacked_states)\n",
    "    a, q = dqn_model.select_action(state.unsqueeze(0))\n",
    "    stacked_states, r, end, info = env.step(a)\n",
    "    env.render() \n",
    "    sleep(.01)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentes artesanales pre-entrenados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Estos agentes corresponden a la última iteración, \n",
    "# pero en realidad debería grabarse el que obtiene mejor recompensa \n",
    "\n",
    "# Pong entrenado durante 200_000 pasos con memoria de 10_000, aprox 2hrs de entrenamiento\n",
    "env_name = \"PongNoFrameskip-v4\"\n",
    "with open(\"modelos/pong_masomenos.pkl\", \"rb\") as f:\n",
    "    dqn_model_loaded, q_policy_state_dict_loaded = pickle.load(f)\n",
    "\n",
    "# Space invaders entrenado por 100_000 con memoria de 10_000\n",
    "#env_name = \"SpaceInvadersNoFrameskip-v4\"\n",
    "#with open(\"modelos/space_invaders_pobre.pkl\", \"rb\") as f:\n",
    "#    dqn_model_loaded, q_policy_state_dict_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "\n",
    "random_agent = False\n",
    "env = wrap_env(gym.make(env_name), skip=1)\n",
    "stacked_states = env.reset()\n",
    "end = False\n",
    "\n",
    "while not end:\n",
    "    if not random_agent:\n",
    "        state = preprocess(stacked_states)\n",
    "        a, q = dqn_model_loaded.select_action(state.unsqueeze(0))\n",
    "    else:\n",
    "        a = env.action_space.sample()\n",
    "    stacked_states, r, end, info = env.step(a)\n",
    "    env.render() \n",
    "    sleep(.01)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
