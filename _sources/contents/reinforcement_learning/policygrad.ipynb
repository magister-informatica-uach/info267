{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitaciones de DQN\n",
    "\n",
    "Las Deep Q Networks (y Q-Learning) tienen dos limitaciones importantes\n",
    "- Espacio de acción discreto (y espacio de estados discreto)\n",
    "- Requieren heurísticas para que haya exploración \n",
    "\n",
    "Estos métodos se basaban en estimar la función **Q** \n",
    "\n",
    "Luego se usa una política determinista de máximas utilidades\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\text{arg}\\max_a' Q(s, a')\n",
    "$$\n",
    "\n",
    "Una alternativa es aprender directamente la *policy*. Veremos como esto puede resolver las limitaciones que mencionamos al principio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy learning\n",
    "\n",
    "En lugar de aprender **Q** podemos aprender directamente\n",
    "\n",
    "$$\n",
    "a \\sim \\pi_\\theta (s)\n",
    "$$\n",
    "\n",
    "- La política es estocástica, es decir sigue una distribución de probabilidad\n",
    "- La acción se muestrea de la política: Esto nos da exploración\n",
    "\n",
    "Podemos usar una red neuronal para modelar los parámetros de la distribución\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "Espacio de acción discreto con 3 opciones\n",
    "\n",
    "Usamos una política con distribución categórica\n",
    "\n",
    "Usamos una red neuronal con tres salidas correspondientes a $p(a_1|s)$, $p(a_2|s)$  y $p(a_3|s)$, donde cada una está en el rango $[0,1]$  y además suman uno: **salida softmax**\n",
    "\n",
    "Luego muestreamos de la distribución categórica usando los parámetros entregados por la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multinomial\n",
    "\n",
    "neural_net_output = np.array([1/3, 1/3, 1/3])\n",
    "#neural_net_output = np.array([0.7, 0.1, 0.2])\n",
    "\n",
    "samples = multinomial(n=1, p=neural_net_output).rvs(10000)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "ax.hist(np.argmax(samples, axis=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "\n",
    "Espacio de acción con dos acciones continuas\n",
    "\n",
    "Usamos una política con distribución Gaussiana\n",
    "\n",
    "Usamos una red neuronal con cuatro salidas (dos por acción) correspondientes a $\\mu(a_1|s)$, $\\log \\sigma(a_1|s)$, $\\mu(a_2|s)$ y $\\log \\sigma(a_2|s)$, donde cada una está en el rango de los reales: **salida lineal**\n",
    "\n",
    "Luego muestreamos de la distribución Gaussiana usando los parámetros entregados por la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "neural_net_output = np.array([0.5, 2., np.log(0.5), np.log(3.)])\n",
    "\n",
    "samples = multivariate_normal(mean=neural_net_output[:2], \n",
    "                              cov=np.diag(np.exp(neural_net_output[2:]))).rvs(10000)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "ax.hist(samples[:, 0], bins=100, alpha=0.5, density=True);\n",
    "ax.hist(samples[:, 1], bins=100, alpha=0.5, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de entrenamiento\n",
    "\n",
    "El algoritmo que usaremos para entrenar busca lo siguiente:\n",
    "- Correr política hasta el término del episodio, grabando las tuplas de acción/estado/recompensa\n",
    "- Disminuir la probabilidad de las acciones que terminaron en recompensa baja\n",
    "- Aumentar la probabilidad de las acciones que terminaron en recompensa alta\n",
    "\n",
    "Esto se peude resumir con la siguiente loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(s_t, a_t, r_t, \\mathcal{H}) = - \\log P(a_t|s_t) G_t\n",
    "$$\n",
    "\n",
    "donde\n",
    "- Log verosimilitud $\\log P(a_t|s_t)$: mide que tan posible es haber seleccionado $a_t$\n",
    "- Ganancia total descontada $G_t$: La ganacia recibida por seleccionar $a_t$\n",
    "\n",
    "Si entrenamos usando gradiente descedente entonces\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\eta \\nabla \\log P(a_t|s_t) G_t\n",
    "$$\n",
    "\n",
    "De donde sale el nombre de este algoritmo: *policy gradient*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Modelo para los parámetros de la política\n",
    "class MultilayerPerceptron(torch.nn.Module):    \n",
    "    def __init__(self, n_input, n_output, n_hidden=10):\n",
    "        super(type(self), self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.linear2 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear3 = torch.nn.Linear(n_hidden, n_output)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.linear1(x))\n",
    "        h = self.activation(self.linear2(h))\n",
    "        return  self.linear3(h)\n",
    "    \n",
    "# Memoría para guardar lo que ocurre en un episodio\n",
    "class Memory:\n",
    "    def __init__(self): \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self): \n",
    "        self.s = []\n",
    "        self.a = []\n",
    "        self.r = []\n",
    "\n",
    "    def push(self, s, a, r): \n",
    "        self.s.append(s)\n",
    "        self.a.append(a)\n",
    "        self.r.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "\n",
    "fig, ax = plt.subplots(2, figsize=(6, 3), sharex=True, tight_layout=True)\n",
    "\n",
    "def smooth_data(x, window_length=10):\n",
    "    return convolve(x, np.ones(window_length)/window_length, mode='valid')\n",
    "\n",
    "def update_plot(step, episode, smooth_window=10, target=195, target_update=500):\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "    episodes = np.arange((episode))\n",
    "    ax[0].scatter(episodes[:episode], diagnostics['rewards'][:episode], s=1)      \n",
    "    if episode > smooth_window:\n",
    "        ax[0].plot(episodes[:-smooth_window+1], \n",
    "                   smooth_data(diagnostics['rewards'][:episode]), alpha=0.5, lw=2)        \n",
    "        #ax[1].plot(episodes[:-smooth_window+1], \n",
    "        #           smooth_data(diagnostics['loss'][:episode]))\n",
    "    ax[1].plot(episodes, diagnostics['loss'][:episode])\n",
    "    ax[0].plot(episodes, [target]*len(episodes), 'k--')\n",
    "    ax[0].set_ylabel('Recompensa');\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].set_xlabel('Episodios')\n",
    "    ax[0].set_title(\"Paso %d\" % (step))\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "n_state = env.observation_space.shape[0] # Número de estados\n",
    "n_action = env.action_space.n # Número de acciones\n",
    "\n",
    "model = MultilayerPerceptron(n_state, n_action)\n",
    "\n",
    "gamma = 0.999\n",
    "loglikelihood = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "memory = Memory()\n",
    "\n",
    "num_episodes = 3000\n",
    "diagnostics = {'rewards': np.zeros(shape=(num_episodes,)), \n",
    "               'loss': np.zeros(shape=(num_episodes,)),\n",
    "               'target_updates': []}\n",
    "\n",
    "global_step_counter = 0\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    end = False\n",
    "    \n",
    "    while not end:        \n",
    "        # Escoger acción\n",
    "        state = torch.tensor(state).float()  \n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(state)\n",
    "            probs = torch.nn.Softmax(dim=0)(logits).numpy()\n",
    "        a = np.random.choice(n_action, size=1, p=probs)[0]\n",
    "        # Ejecutar acción\n",
    "        s, r, end, info = env.step(a)\n",
    "        # Guardar en memoria\n",
    "        memory.push(state, torch.tensor(a, dtype=int), torch.tensor(r))\n",
    "        # Preparar para próximo paso\n",
    "        diagnostics['rewards'][episode] += r\n",
    "        state = s\n",
    "        global_step_counter +=1\n",
    "        \n",
    "    # Una vez completado el episodio actualizamos el modelo\n",
    "    \n",
    "    # Primero calculamos la recompensa total descontada\n",
    "    Gt = (torch.stack(memory.r)*gamma**torch.arange(0, len(memory.r)).float()).flip(0).cumsum(0).flip(0)\n",
    "    # Luego la normalizamos (mejora la estabilidad del entrenamiento)\n",
    "    Gt = (Gt - Gt.mean())/Gt.std()\n",
    "    # Predicción (sin normalizar) de la probabilidad de acción\n",
    "    logits = model.forward(torch.stack(memory.s))\n",
    "    # Cálculo de la loss\n",
    "    loss = torch.mean(loglikelihood(logits, torch.stack(memory.a))*Gt)\n",
    "    # Calculamos los gradientes y actualizamos los parámetros\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()     \n",
    "    diagnostics['loss'][episode] += loss\n",
    "    # Borramos la memoria\n",
    "    memory.reset()\n",
    "                \n",
    "    if episode % 25 == 0:\n",
    "        update_plot(global_step_counter, episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agente en acción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "end = False\n",
    "\n",
    "for k in range(500):\n",
    "    env.render()\n",
    "    state = torch.from_numpy(np.array(env.state).astype('float32'))\n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(state)\n",
    "        probs = torch.nn.Softmax(dim=0)(logits).numpy()\n",
    "    a = np.random.choice(2, size=1, p=probs)[0]\n",
    "    s_future, r, end, info = env.step(a)\n",
    "    #if end:\n",
    "    #    break\n",
    "    if r == 0:\n",
    "        display(k)\n",
    "        break\n",
    "display(end, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaGo \n",
    "\n",
    "Go es un juego de mesa de origen chino con un tablero de 19x19 espacios\n",
    "\n",
    "Dos jugadores compiten por cubrir la mayor área en el tablero\n",
    "\n",
    "El espacio de estados es tiene [más movimientos legales que átomos en el universe](https://en.wikipedia.org/wiki/Go_and_mathematics#Legal_positions)\n",
    "\n",
    "El equipo de DeepMind ha utilizado Go como benchmark para proponer nuevos modelos de RL profundo\n",
    "\n",
    "El primero de ellos es Alpha Go (2016) el cual combinó bastantes técnicas\n",
    "1. Se entrena una red neuronal supervisada con movimientos de humanos expertos: Imitar a los humanos\n",
    "1. Se entrena un agente basado en política que compite contra el modelo entrenado: Aprender nuevos movimientos\n",
    "1. Una vez que el agente supera al modelo supervisado se sigue entrenando contra si mismo\n",
    "1. Adicionalmente se ocupa un modelo basado en valor para evaluar los movimientos del agente\n",
    "\n",
    "[Presentación autocontenida sobre Alpha Go](https://www.slideshare.net/ckmarkohchang/alphago-in-depth)\n",
    "\n",
    "Alpha Go Zero no usa pre-entrenamiento con redes supervisadas\n",
    "\n",
    "Alpha Zero extiende a otros juegos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafíos de Aprendizaje Reforzado\n",
    "\n",
    "- Para entrenar con *policy gradient* necesitamos correr el agente hasta el término del episodio\n",
    "- Esto en muchas cosas significa la muerte del agente\n",
    "- ¿Cómo podemos desarrollar un agente en el mundo real?\n",
    "\n",
    "Una alternativa es usar simuladores\n",
    "- Usar un simulación realista del ambiente para entrenar, e.g. [VISTA](http://www.mit.edu/~amini/vista/)\n",
    "- Hacer transferencia de aprendizaje desde el simulador al mundo real (por las imperfecciones del simulador)\n",
    "\n",
    "Otra alternativa es one-shot learning\n",
    "- [One-shot learning of manipulation skills with online dynamics adaptation and neural network priors](http://rll.berkeley.edu/iros2016onlinecontrol/online_control.pdf)\n",
    "\n",
    "Y otra alternativa más es usar apoyo humano\n",
    "- [Trial without Error: Towards Safe Reinforcement Learning via Human Intervention](https://arxiv.org/abs/1707.05173) y [blog post](https://owainevans.github.io/blog/hirl_blog.html)\n",
    "\n",
    "\n",
    "Otro desafío general de RL es el proponer buenas funciones de recompensa. Este y otros desafios abiertos se discuten en esta [\n",
    "fuerte crítica a RL](https://www.alexirpan.com/2018/02/14/rl-hard.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
